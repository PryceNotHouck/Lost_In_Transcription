{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from typing import Dict, Any"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd2e39ef1e61e92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:47:53.787231Z",
     "start_time": "2025-09-27T18:47:53.781398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Audio with sample rate 16,000, standard for CrisperWhisper with librosa\n",
    "\n",
    "def load_audio(path: str, target_sr: int = 16000):\n",
    "    audio, sr = librosa.load(path, sr=None, mono=True)\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "    return audio, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc90910c26cc339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:47:54.826814Z",
     "start_time": "2025-09-27T18:47:54.820049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create audio chunks of a specified length in seconds with some overlap\n",
    "\n",
    "def chunk_audio(audio: \"np.ndarray\", sr: int, chunk_s: int, overlap_s: float = 0.5):\n",
    "    step = int((chunk_s - overlap_s) * sr)\n",
    "    chunk_len = int(chunk_s * sr)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(audio):\n",
    "        end = min(start + chunk_len, len(audio))\n",
    "        chunk = audio[start:end]\n",
    "        chunks.append((chunk, start / sr))  # (samples, start_time_seconds)\n",
    "        if end == len(audio):\n",
    "            break\n",
    "        start += step\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19895bf22ed51c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:47:56.492028Z",
     "start_time": "2025-09-27T18:47:56.473023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Format pipeline outputs into readable verbatim transcript with word level timestamps\n",
    "\n",
    "# walk through in debug after the pipeline is implemented to eliminate unnecessary functionality\n",
    "def format_verbatim_output(hf_out: Dict[str, Any]):\n",
    "    # hf_out expected to include 'text' and 'chunks' where each chunk has 'timestamps' at word level\n",
    "    lines = []\n",
    "    for chunk in hf_out.get(\"chunks\", []):\n",
    "        chunk_start = chunk.get(\"timestamp\", (None, None))[0]\n",
    "        words = chunk.get(\"words\") or chunk.get(\"timestamps\") or []\n",
    "        for w in words:\n",
    "            # try to support both shapes\n",
    "            if isinstance(w, dict):\n",
    "                word_text = w.get(\"word\") or w.get(\"text\")\n",
    "                ts = w.get(\"timestamp\") or w.get(\"times\") or w.get(\"start_end\")\n",
    "                if isinstance(ts, (list, tuple)) and len(ts) >= 2:  # if the pipeline stores words with timestamp tuples\n",
    "                    start_ts = ts[0]\n",
    "                else:\n",
    "                    start_ts = None\n",
    "            elif isinstance(w, (list, tuple)) and len(w) >= 3:  # [word, start, end]\n",
    "                word_text = w[0]\n",
    "                start_ts = w[1]\n",
    "            else:  # fallback\n",
    "                word_text = str(w)\n",
    "                start_ts = None\n",
    "            if start_ts is None:\n",
    "                line = f\"{word_text}\"\n",
    "            else:\n",
    "                line = f\"[{start_ts:.3f}] {word_text}\"\n",
    "            lines.append(line)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22546704cbefed2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:49:46.487407Z",
     "start_time": "2025-09-27T19:49:46.480313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef219de9645ce201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:53:33.632440Z",
     "start_time": "2025-09-27T18:53:28.764593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if \"cuda\" in device else torch.float32\n",
    "model_id = \"nyrahealth/CrisperWhisper\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch_dtype,\n",
    "    low_cpu_mem_usage = True,\n",
    "    use_safetensors = True,\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model = model,\n",
    "    tokenizer = processor.tokenizer,\n",
    "    feature_extractor = processor.feature_extractor,\n",
    "    chunk_length_s = 30,  # on model card\n",
    "    batch_size = 16,  # on model card\n",
    "    return_timestamps = \"word\",\n",
    "    device = 0 if \"cuda\" in device else -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c3585d28a7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(\n",
    "    audio_path: str,\n",
    "    asr_pipeline,\n",
    "    chunk_length_s: int = 30,\n",
    "    overlap_s: float = 0.5,\n",
    ") -> str:\n",
    "    audio, sr = load_audio(audio_path, target_sr = 16000)\n",
    "    audio_chunks = chunk_audio(audio, sr, chunk_s = chunk_length_s, overlap_s = overlap_s)\n",
    "\n",
    "    full_chunks = []\n",
    "    for chunk_samples, chunk_start in audio_chunks:\n",
    "        sample = {\"array\": chunk_samples, \"sampling_rate\": sr}\n",
    "        hf_out = asr_pipeline(sample)\n",
    "        # shift timestamps by chunk_start\n",
    "        if \"chunks\" in hf_out:\n",
    "            for c in hf_out[\"chunks\"]:\n",
    "                if \"words\" in c:\n",
    "                    for w in c[\"words\"]:\n",
    "                        ts = w.get(\"timestamp\") or w.get(\"times\") or w.get(\"start_end\")\n",
    "                        if isinstance(ts, (list, tuple)) and len(ts) >= 2:\n",
    "                            # shift both start and end timestamps\n",
    "                            w_ts0 = ts[0] + chunk_start\n",
    "                            w_ts1 = ts[1] + chunk_start\n",
    "                            w[\"timestamp\"] = (w_ts0, w_ts1)\n",
    "        hf_out[\"_chunk_start\"] = chunk_start\n",
    "        full_chunks.append(hf_out)\n",
    "\n",
    "    combined = {\"chunks\": []}\n",
    "    for out in full_chunks:\n",
    "        if \"chunks\" in out:\n",
    "            combined[\"chunks\"].extend(out[\"chunks\"])\n",
    "        else:\n",
    "            if \"timestamps\" in out:\n",
    "                combined[\"chunks\"].append ({\n",
    "                        \"timestamp\": (out.get(\"_chunk_start\", 0), None),\n",
    "                        \"words\": out[\"timestamps\"],\n",
    "                    })\n",
    "            else:\n",
    "                combined[\"chunks\"].append ({\n",
    "                    \"timestamp\": (out.get(\"_chunk_start\", 0), None),\n",
    "                    \"words\": [{\"word\": out.get(\"text\", \"\").strip()\n",
    "                    }]}\n",
    "                )\n",
    "\n",
    "    transcript = format_verbatim_output(combined)\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b354cda11270d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = r\"data/audio/...\"\n",
    "output_txt = r\"data/WER0/...\"\n",
    "\n",
    "print(\"Transcribing:\", audio_file)\n",
    "transcript_text = transcribe_audio(audio_file, asr_pipe, chunk_length_s = 30, overlap_s = 0.5)\n",
    "\n",
    "# display first 50 lines\n",
    "print(\"---- Transcript preview ----\")\n",
    "print(\"\\n\".join(transcript_text.splitlines()[:50]))\n",
    "print(\"... (truncated)\")\n",
    "\n",
    "# save\n",
    "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(transcript_text)\n",
    "\n",
    "print(f\"Saved full transcript to: {output_txt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIT Kernel",
   "language": "python",
   "name": "lit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
