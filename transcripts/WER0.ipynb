{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T18:31:38.339653Z",
     "start_time": "2025-10-16T18:31:31.195050Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from typing import Dict, Any\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import ffmpeg"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:31:42.950910Z",
     "start_time": "2025-10-16T18:31:42.944915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transcribe_audio(file_path):\n",
    "    print(\"Starting transcription...\")\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    #model_id = \"/gpfs/project/mukha102/CrisperWhisper/nyra_model\"\n",
    "    model_id = \"nyrahealth/CrisperWhisper\"\n",
    "\n",
    "    # Load model and processor with eager attention implementation\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype = torch_dtype,\n",
    "        low_cpu_mem_usage = True,\n",
    "        use_safetensors = True,\n",
    "        attn_implementation = \"eager\"\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model = model,\n",
    "        tokenizer = processor.tokenizer,\n",
    "        feature_extractor = processor.feature_extractor,\n",
    "        chunk_length_s = 30,\n",
    "        batch_size = 1,  # Reduced batch size\n",
    "        return_timestamps = \"word\",\n",
    "        torch_dtype = torch_dtype,\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    result = pipe(file_path)\n",
    "    torch.cuda.empty_cache()  # Free up memory after each transcription\n",
    "    return result"
   ],
   "id": "75b18ca899cefa0b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:32:05.815597Z",
     "start_time": "2025-10-16T18:32:01.282251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === CELL 3: Run transcription interactively in Jupyter ===\n",
    "\n",
    "# You can manually set your audio file path here:\n",
    "file_path = r\"C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\data\\audio\\AAHP 005A Mattie Williams 1-16-2010.mp3\"  # <-- Replace with your file path\n",
    "\n",
    "# Optionally: Use an upload widget for convenience\n",
    "# from IPython.display import display\n",
    "# import ipywidgets as widgets\n",
    "# uploader = widgets.FileUpload(accept='.mp3,.wav', multiple=False)\n",
    "# display(uploader)\n",
    "\n",
    "# --- Validation ---\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "\n",
    "# --- Run transcription ---\n",
    "try:\n",
    "    transcription = transcribe_audio(file_path)\n",
    "\n",
    "    # Save the transcription output as a JSON file next to the audio\n",
    "    output_path = file_path.rsplit(\".\", 1)[0] + \"_transcription.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        import json\n",
    "        json.dump(transcription, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Transcription complete! Saved to {output_path}\")\n",
    "\n",
    "    # Display a summary of the result directly in the notebook\n",
    "    if isinstance(transcription, dict) and \"text\" in transcription:\n",
    "        print(\"\\n--- TRANSCRIPTION PREVIEW ---\\n\")\n",
    "        print(transcription[\"text\"][:1000])  # show first 1000 chars\n",
    "    else:\n",
    "        print(\"\\nTranscription output structure:\\n\", transcription)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while transcribing the audio: {e}\")\n"
   ],
   "id": "5372a1fb05807ee2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ An error occurred while transcribing the audio: ffmpeg was not found but is required to load audio files from filename\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7dc3fec0a6db9ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:44.471015Z",
     "start_time": "2025-10-06T21:25:36.276439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if \"cuda\" in device else torch.float32\n",
    "model_id = \"nyrahealth/CrisperWhisper\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch_dtype,\n",
    "    low_cpu_mem_usage = True,\n",
    "    use_safetensors = True,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    task = \"automatic-speech-recognition\",\n",
    "    model = model,\n",
    "    tokenizer = processor.tokenizer,\n",
    "    feature_extractor = processor.feature_extractor,\n",
    "    chunk_length_s = 30,  # on model card\n",
    "    batch_size = 16,  # on model card\n",
    "    return_timestamps = \"word\",\n",
    "    device = 0 if \"cuda\" in device else -1,\n",
    ")"
   ],
   "id": "9a2c50e0f07b0a48",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:44.892860Z",
     "start_time": "2025-10-06T21:25:44.884543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ],
   "id": "954b274420a77a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.8.0+cu128\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:44.952805Z",
     "start_time": "2025-10-06T21:25:44.945962Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.config.max_length)",
   "id": "2e04260550a9f559",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:44.999956Z",
     "start_time": "2025-10-06T21:25:44.990586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Audio with sample rate 16,000, standard for CrisperWhisper with librosa\n",
    "\n",
    "def load_audio(path: str, target_sr: int = 16000):\n",
    "    audio, sr = librosa.load(path, sr = None, mono = True)\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr = sr, target_sr = target_sr)\n",
    "        sr = target_sr\n",
    "    return audio, sr"
   ],
   "id": "e7f3962e05ff8d75",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:45.025024Z",
     "start_time": "2025-10-06T21:25:45.014123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create audio chunks of a specified length in seconds with some overlap\n",
    "\n",
    "def chunk_audio(audio: \"np.ndarray\", sr: int, chunk_s: int, overlap_s: float = 0.5):\n",
    "    step = int((chunk_s - overlap_s) * sr)\n",
    "    chunk_len = int(chunk_s * sr)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(audio):\n",
    "        end = min(start + chunk_len, len(audio))\n",
    "        chunk = audio[start:end]\n",
    "\n",
    "        # pad chunk with zeroes if it's shorter than chunk_len\n",
    "        if end == len(audio):\n",
    "            pad = [float(0) for i in range((start + chunk_len) - len(audio))]\n",
    "            chunk = np.append(chunk, pad)\n",
    "        chunks.append((chunk, start / sr))  # (samples, start_time_seconds)\n",
    "\n",
    "        if end == len(audio):\n",
    "            break\n",
    "        start += step\n",
    "    return chunks"
   ],
   "id": "e9b4e13933c5414a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:45.047676Z",
     "start_time": "2025-10-06T21:25:45.036680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Format pipeline outputs into readable verbatim transcript with word level timestamps\n",
    "\n",
    "# walk through in debug after the pipeline is implemented to eliminate unnecessary functionality\n",
    "def format_verbatim_output(hf_out: Dict[str, Any]):\n",
    "    # hf_out expected to include 'text' and 'chunks' where each chunk has 'timestamps' at word level\n",
    "    lines = []\n",
    "    for chunk in hf_out.get(\"chunks\", []):\n",
    "        chunk_start = chunk.get(\"timestamp\", (None, None))[0]\n",
    "        words = chunk.get(\"words\") or chunk.get(\"timestamps\") or []\n",
    "        for w in words:\n",
    "            # try to support both shapes\n",
    "            if isinstance(w, dict):\n",
    "                word_text = w.get(\"word\") or w.get(\"text\")\n",
    "                ts = w.get(\"timestamp\") or w.get(\"times\") or w.get(\"start_end\")\n",
    "                if isinstance(ts, (list, tuple)) and len(ts) >= 2:  # if the pipeline stores words with timestamp tuples\n",
    "                    start_ts = ts[0]\n",
    "                else:\n",
    "                    start_ts = None\n",
    "            elif isinstance(w, (list, tuple)) and len(w) >= 3:  # [word, start, end]\n",
    "                word_text = w[0]\n",
    "                start_ts = w[1]\n",
    "            else:  # fallback\n",
    "                word_text = str(w)\n",
    "                start_ts = None\n",
    "            if start_ts is None:\n",
    "                line = f\"{word_text}\"\n",
    "            else:\n",
    "                line = f\"[{start_ts:.3f}] {word_text}\"\n",
    "            lines.append(line)\n",
    "    return \"\\n\".join(lines)"
   ],
   "id": "81c8b2bb5ba7548d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "dd1c3585d28a7f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:25:45.084605Z",
     "start_time": "2025-10-06T21:25:45.058967Z"
    }
   },
   "source": [
    "def transcribe_audio(\n",
    "    audio_path: str,\n",
    "    asr_pipeline,\n",
    "    chunk_length_s: int = 30,\n",
    "    overlap_s: float = 0.5,\n",
    ") -> str:\n",
    "    audio, sr = load_audio(audio_path, target_sr = 16000)\n",
    "    audio_chunks = chunk_audio(audio, sr, chunk_s = chunk_length_s, overlap_s = overlap_s)\n",
    "\n",
    "    print(\"Chunks:\", len(audio_chunks))\n",
    "    print(\"Chunk Size:\", chunk_length_s * sr)\n",
    "    for chunk, start in audio_chunks:\n",
    "        if chunk.size != chunk_length_s * sr:\n",
    "            print(\"Bad Size:\", chunk.size)\n",
    "\n",
    "    full_chunks = []\n",
    "    for chunk, start in audio_chunks:\n",
    "        sample = {\"array\": chunk.astype(\"float32\"), \"sampling_rate\": sr}\n",
    "        hf_out = asr_pipeline(sample)\n",
    "        print(hf_out)\n",
    "\n",
    "        # shift timestamps by chunk_start\n",
    "        if \"chunks\" in hf_out:\n",
    "            for c in hf_out[\"chunks\"]:\n",
    "                if \"words\" in c:\n",
    "                    for w in c[\"words\"]:\n",
    "                        ts = w.get(\"timestamp\") or w.get(\"times\") or w.get(\"start_end\")\n",
    "                        if isinstance(ts, (list, tuple)) and len(ts) >= 2:\n",
    "                            # shift both start and end timestamps\n",
    "                            w_ts0 = ts[0] + start\n",
    "                            w_ts1 = ts[1] + start\n",
    "                            w[\"timestamp\"] = (w_ts0, w_ts1)\n",
    "        hf_out[\"_chunk_start\"] = start\n",
    "        full_chunks.append(hf_out)\n",
    "\n",
    "    combined = {\"chunks\": []}\n",
    "    for out in full_chunks:\n",
    "        if \"chunks\" in out:\n",
    "            combined[\"chunks\"].extend(out[\"chunks\"])\n",
    "        else:\n",
    "            if \"timestamps\" in out:\n",
    "                combined[\"chunks\"].append ({\n",
    "                        \"timestamp\": (out.get(\"_chunk_start\", 0), None),\n",
    "                        \"words\": out[\"timestamps\"],\n",
    "                    })\n",
    "            else:\n",
    "                combined[\"chunks\"].append ({\n",
    "                    \"timestamp\": (out.get(\"_chunk_start\", 0), None),\n",
    "                    \"words\": [{\"word\": out.get(\"text\", \"\").strip()\n",
    "                    }]}\n",
    "                )\n",
    "\n",
    "    transcript = format_verbatim_output(combined)\n",
    "    return transcript"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "16b354cda11270d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:26:03.997867Z",
     "start_time": "2025-10-06T21:25:45.104554Z"
    }
   },
   "source": [
    "audio_file = r\"C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\data\\audio\\AAHP 005A Mattie Williams 1-16-2010.mp3\"\n",
    "output_txt = r\"C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\data\\WER0\\001_test.txt\"\n",
    "\n",
    "print(\"Transcribing:\", audio_file)\n",
    "transcript_text = transcribe_audio(audio_file, asr_pipe, chunk_length_s = 30, overlap_s = 0.5)\n",
    "\n",
    "# display first 50 lines\n",
    "print(\"---- Transcript preview ----\")\n",
    "print(\"\\n\".join(transcript_text.splitlines()[:50]))\n",
    "print(\"... (truncated, first 50 lines)\")\n",
    "\n",
    "# save\n",
    "with open(output_txt, \"w\", encoding = \"utf-8\") as f:\n",
    "    f.write(transcript_text)\n",
    "\n",
    "print(f\"Saved full transcript to: {output_txt}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\data\\audio\\AAHP 001Rosa Williams 4-28-2009.mp3\n",
      "Chunks: 86\n",
      "Chunk Size: 480000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m output_txt = \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC:\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mUsers\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mpryce\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mPycharmProjects\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mLostInTranscription\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mWER0\u001B[39m\u001B[33m\\\u001B[39m\u001B[33m001_test.txt\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTranscribing:\u001B[39m\u001B[33m\"\u001B[39m, audio_file)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m transcript_text = \u001B[43mtranscribe_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43masr_pipe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_length_s\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverlap_s\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# display first 50 lines\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m---- Transcript preview ----\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mtranscribe_audio\u001B[39m\u001B[34m(audio_path, asr_pipeline, chunk_length_s, overlap_s)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m chunk, start \u001B[38;5;129;01min\u001B[39;00m audio_chunks:\n\u001B[32m     18\u001B[39m     sample = {\u001B[33m\"\u001B[39m\u001B[33marray\u001B[39m\u001B[33m\"\u001B[39m: chunk.astype(\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m), \u001B[33m\"\u001B[39m\u001B[33msampling_rate\u001B[39m\u001B[33m\"\u001B[39m: sr}\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     hf_out = \u001B[43masr_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m     \u001B[38;5;28mprint\u001B[39m(hf_out)\n\u001B[32m     22\u001B[39m     \u001B[38;5;66;03m# shift timestamps by chunk_start\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:275\u001B[39m, in \u001B[36mAutomaticSpeechRecognitionPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    218\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: Union[np.ndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mdict\u001B[39m], **kwargs: Any) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    219\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    220\u001B[39m \u001B[33;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001B[39;00m\n\u001B[32m    221\u001B[39m \u001B[33;03m    documentation for more information.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    273\u001B[39m \u001B[33;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001B[39;00m\n\u001B[32m    274\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m275\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\base.py:1459\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1457\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[32m   1458\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[32m-> \u001B[39m\u001B[32m1459\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m   1460\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m   1461\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_iterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1462\u001B[39m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\n\u001B[32m   1463\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1464\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1465\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1466\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1467\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    123\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_item()\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m item = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m processed = \u001B[38;5;28mself\u001B[39m.infer(item, **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:271\u001B[39m, in \u001B[36mPipelinePackIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    268\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[32m    270\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[32m--> \u001B[39m\u001B[32m271\u001B[39m     processed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    272\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    273\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch.Tensor):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1372\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1373\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1374\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1375\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:535\u001B[39m, in \u001B[36mAutomaticSpeechRecognitionPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001B[39m\n\u001B[32m    529\u001B[39m main_input_name = \u001B[38;5;28mself\u001B[39m.model.main_input_name \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.model, \u001B[33m\"\u001B[39m\u001B[33mmain_input_name\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33minputs\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    530\u001B[39m generate_kwargs = {\n\u001B[32m    531\u001B[39m     main_input_name: inputs,\n\u001B[32m    532\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m\"\u001B[39m: attention_mask,\n\u001B[32m    533\u001B[39m     **generate_kwargs,\n\u001B[32m    534\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m535\u001B[39m tokens = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001B[39;00m\n\u001B[32m    538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_timestamps == \u001B[33m\"\u001B[39m\u001B[33mword\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.type == \u001B[33m\"\u001B[39m\u001B[33mseq2seq_whisper\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:866\u001B[39m, in \u001B[36mWhisperGenerationMixin.generate\u001B[39m\u001B[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001B[39m\n\u001B[32m    857\u001B[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001B[32m1\u001B[39m])\n\u001B[32m    859\u001B[39m \u001B[38;5;66;03m# 6.6 Run generate with fallback\u001B[39;00m\n\u001B[32m    860\u001B[39m (\n\u001B[32m    861\u001B[39m     seek_sequences,\n\u001B[32m    862\u001B[39m     seek_outputs,\n\u001B[32m    863\u001B[39m     should_skip,\n\u001B[32m    864\u001B[39m     do_condition_on_prev_tokens,\n\u001B[32m    865\u001B[39m     model_output_type,\n\u001B[32m--> \u001B[39m\u001B[32m866\u001B[39m ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_with_fallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m    \u001B[49m\u001B[43msegment_input\u001B[49m\u001B[43m=\u001B[49m\u001B[43msegment_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    869\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcur_bsz\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcur_bsz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    870\u001B[39m \u001B[43m    \u001B[49m\u001B[43mseek\u001B[49m\u001B[43m=\u001B[49m\u001B[43mseek\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    871\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_idx_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_idx_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    872\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtemperatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    873\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprefix_allowed_tokens_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefix_allowed_tokens_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m    \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_timestamps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_timestamps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_condition_on_prev_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_condition_on_prev_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_shortform\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_shortform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    886\u001B[39m \u001B[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001B[39;00m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, seek_sequence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(seek_sequences):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1053\u001B[39m, in \u001B[36mWhisperGenerationMixin.generate_with_fallback\u001B[39m\u001B[34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001B[39m\n\u001B[32m   1050\u001B[39m model_output_type = \u001B[38;5;28mtype\u001B[39m(seek_outputs)\n\u001B[32m   1052\u001B[39m \u001B[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1053\u001B[39m seek_sequences, seek_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_postprocess_outputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m    \u001B[49m\u001B[43mseek_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mseek_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_timestamps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_timestamps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1057\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_shortform\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_shortform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1059\u001B[39m \u001B[43m    \u001B[49m\u001B[43mseek\u001B[49m\u001B[43m=\u001B[49m\u001B[43mseek\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1060\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_idx_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_idx_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1061\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1063\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cur_bsz < batch_size:\n\u001B[32m   1064\u001B[39m     seek_sequences = seek_sequences[:cur_bsz]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1163\u001B[39m, in \u001B[36mWhisperGenerationMixin._postprocess_outputs\u001B[39m\u001B[34m(self, seek_outputs, decoder_input_ids, return_token_timestamps, generation_config, is_shortform, seek, batch_idx_map)\u001B[39m\n\u001B[32m   1160\u001B[39m         num_frames = num_frames - seek\n\u001B[32m   1161\u001B[39m         num_frames = num_frames[batch_idx_map]\n\u001B[32m-> \u001B[39m\u001B[32m1163\u001B[39m     seek_outputs[\u001B[33m\"\u001B[39m\u001B[33mtoken_timestamps\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_extract_token_timestamps\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1164\u001B[39m \u001B[43m        \u001B[49m\u001B[43mseek_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43malignment_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1166\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1167\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1168\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1170\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msplit_by_batch_index\u001B[39m(values, key, batch_idx, is_shortform, beam_indices=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   1171\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m beam_indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m key == \u001B[33m\"\u001B[39m\u001B[33mscores\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:285\u001B[39m, in \u001B[36mWhisperGenerationMixin._extract_token_timestamps\u001B[39m\u001B[34m(self, generate_outputs, alignment_heads, time_precision, num_frames, num_input_ids)\u001B[39m\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m num_input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m num_input_ids > \u001B[32m1\u001B[39m:\n\u001B[32m    282\u001B[39m     \u001B[38;5;66;03m# `-1`: `beam_indices` can be used as-is to gather the weights when `num_input_ids` is 1\u001B[39;00m\n\u001B[32m    283\u001B[39m     weight_length += num_input_ids - \u001B[32m1\u001B[39m\n\u001B[32m    284\u001B[39m     beam_indices_first_step_unrolled = (\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m         \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_input_ids\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    286\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    287\u001B[39m     )\n\u001B[32m    288\u001B[39m     unrolled_beam_indices = torch.cat([beam_indices_first_step_unrolled, beam_indices], dim=-\u001B[32m1\u001B[39m)\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIT Kernel",
   "language": "python",
   "name": "lit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
