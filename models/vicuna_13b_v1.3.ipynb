{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Classification tasks with Vicuna-13B-v1.3 (HF, transformers) over WER-binned sample sentences\n",
        "\n",
        "Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.\n",
        "- Model: [lmsys/vicuna-13b-v1.3](https://huggingface.co/lmsys/vicuna-13b-v1.3)\n",
        "- License: Non-commercial license\n",
        "- Base model: LLaMA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "import logging\n",
        "import random\n",
        "from datetime import datetime\n",
        "import re\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Memory Management Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variable to help with memory fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Clear any existing CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA cache cleared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Model Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"lmsys/vicuna-13b-v1.3\"\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)\n",
        "\n",
        "# Set pad_token for Vicuna models (they don't have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    #device_map = \"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    # TODO: consider quantization settings based on hardware\n",
        ")\n",
        "\n",
        "# Initialize the generation pipeline\n",
        "# Don't use device_map in pipeline when model already has device_map=\"auto\"\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Test Pipeline (Optional - Run this to verify setup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test to verify the pipeline works\n",
        "# This should complete in a few seconds, not minutes\n",
        "print(\"Testing Vicuna pipeline with a simple prompt...\")\n",
        "test_prompt = \"USER: Hello, how are you?\\nASSISTANT:\"\n",
        "test_output = gen_pipeline(\n",
        "    test_prompt,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=False,\n",
        "    return_full_text=False,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "print(f\"Test successful! Output: {test_output[0]['generated_text']}\")\n",
        "print(\"\\nIf this test hangs or takes more than 30 seconds, there's a configuration issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Prompt Structure\n",
        "\n",
        "Vicuna uses a conversation format with USER/ASSISTANT turns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot prompt using Vicuna's conversation format\n",
        "ZERO_SHOT_PROMPT = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "\n",
        "USER: You are an assistant that classifies the sentiment of user utterances. You must respond with three parts:\n",
        "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
        "2) A short explanation (1–2 sentences) of why you chose that label\n",
        "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
        "4) (Optionally) any caveats or uncertainty if applicable\n",
        "\n",
        "Please classify the sentiment of this utterance: \"{sentence}\"\n",
        "ASSISTANT:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot prompt for continuous sentiment (integer scale)\n",
        "ZERO_SHOT_PROMPT_CONTINUOUS = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "\n",
        "USER: You are an assistant that classifies the sentiment of user utterances. You must respond with three parts:\n",
        "1) An integer value for sentiment between -10 and 10, with -10 being the most negative and 10 being the most positive, and a score of 0 being neutral\n",
        "2) A short explanation (1–2 sentences) of why you chose that label\n",
        "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
        "4) (Optionally) any caveats or uncertainty if applicable\n",
        "\n",
        "Please classify the sentiment of this utterance: \"{sentence}\"\n",
        "ASSISTANT:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format for few-shot examples - will be populated dynamically\n",
        "FEW_SHOT_EXAMPLES_TEMPLATE = \"\"\"\n",
        "### EXAMPLES ###\n",
        "{examples}\n",
        "### END EXAMPLES ###\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot prompt template using Vicuna's conversation format\n",
        "FEW_SHOT_PROMPT = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "\n",
        "USER: You are an assistant that classifies the sentiment of user utterances. You must respond with three parts:\n",
        "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
        "2) A short explanation (1–2 sentences) of why you chose that label\n",
        "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
        "4) (Optionally) any caveats or uncertainty if applicable\n",
        "\n",
        "Here are some examples of how to classify sentiment:\n",
        "{examples}\n",
        "\n",
        "Now, please classify the sentiment of this utterance: \"{sentence}\"\n",
        "ASSISTANT:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot prompt for continuous sentiment (integer scale)\n",
        "FEW_SHOT_PROMPT_CONTINUOUS = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "\n",
        "USER: You are an assistant that classifies the sentiment of user utterances. You must respond with three parts:\n",
        "1) An integer value for sentiment between -10 and 10, with -10 being the most negative and 10 being the most positive, and a score of 0 being neutral\n",
        "2) A short explanation (1–2 sentences) of why you chose that label\n",
        "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
        "4) (Optionally) any caveats or uncertainty if applicable\n",
        "\n",
        "Here are some examples of how to classify sentiment:\n",
        "{examples}\n",
        "\n",
        "Now, please classify the sentiment of this utterance: \"{sentence}\"\n",
        "ASSISTANT:\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading and Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Load Data from XLSX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_xlsx(xlsx_path: str, sheet_index: int = 1) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load data from the second sheet (index 1) of an XLSX file.\n",
        "    Expected columns:\n",
        "    - Machine Transcription (ground truth)\n",
        "    - Human Transcription (hypothesis)\n",
        "    - Sentiment Label\n",
        "    - WER\n",
        "    - CER\n",
        "    - List of AAE features\n",
        "    - List of transcription errors\n",
        "    - Interview\n",
        "    \"\"\"\n",
        "    xl_file = pd.ExcelFile(xlsx_path)\n",
        "    sheet_names = xl_file.sheet_names\n",
        "    if len(sheet_names) <= sheet_index:\n",
        "        raise ValueError(f\"Sheet index {sheet_index} not available. Available sheets: {sheet_names}\")\n",
        "    \n",
        "    df = pd.read_excel(xlsx_path, sheet_name=sheet_index)\n",
        "    logging.info(f\"Loaded {len(df)} rows from sheet '{sheet_names[sheet_index]}'\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data_into_sets(df: pd.DataFrame, test_ratio: float = 0.5, random_seed: int = None) -> tuple:\n",
        "    \"\"\"\n",
        "    Split dataframe into test and few-shot example sets.\n",
        "    Returns (test_df, few_shot_df) with equal sizes (or as close as possible).\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "    \n",
        "    # Shuffle the dataframe\n",
        "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
        "    \n",
        "    # Split in half\n",
        "    split_idx = len(df_shuffled) // 2\n",
        "    test_df = df_shuffled.iloc[:split_idx].copy()\n",
        "    few_shot_df = df_shuffled.iloc[split_idx:].copy()\n",
        "    \n",
        "    logging.info(f\"Split data: {len(test_df)} test samples, {len(few_shot_df)} few-shot examples\")\n",
        "    return test_df, few_shot_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classify with Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Classify with Vicuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_few_shot_examples(few_shot_df: pd.DataFrame, num_examples: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Format 5 randomly selected examples from few_shot_df.\n",
        "    Uses Machine Transcription (column 1) for examples as specified.\n",
        "    \"\"\"\n",
        "    if len(few_shot_df) < num_examples:\n",
        "        num_examples = len(few_shot_df)\n",
        "    \n",
        "    # Randomly select examples\n",
        "    selected = few_shot_df.sample(n=num_examples, random_state=None)\n",
        "    \n",
        "    examples_text = []\n",
        "    for idx, (_, row) in enumerate(selected.iterrows(), 1):\n",
        "        # Get column names - handle variations\n",
        "        machine_col = None\n",
        "        human_col = None\n",
        "        sentiment_col = None\n",
        "        \n",
        "        for col in row.index:\n",
        "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
        "                machine_col = col\n",
        "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
        "                human_col = col\n",
        "            elif 'sentiment' in col.lower():\n",
        "                sentiment_col = col\n",
        "        \n",
        "        if machine_col is None or human_col is None:\n",
        "            # Try to find by position or common names\n",
        "            cols = list(row.index)\n",
        "            if len(cols) >= 2:\n",
        "                machine_col = cols[0] if machine_col is None else machine_col\n",
        "                human_col = cols[1] if human_col is None else human_col\n",
        "        \n",
        "        # Use Machine Transcription (column 1) for examples as specified\n",
        "        machine_text = row.get(machine_col, \"N/A\")\n",
        "        sentiment = row.get(sentiment_col, \"N/A\")\n",
        "        \n",
        "        # Format as example for the prompt\n",
        "        example = f\"{idx}. Utterance: \\\"{machine_text}\\\"\\n   Classification: [Sentiment: {sentiment}, Reason: This is an example sentence.]\"\n",
        "        examples_text.append(example)\n",
        "    \n",
        "    return \"\\n\".join(examples_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_sentiment_output(output_text: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Parse sentiment label and reason from model output.\n",
        "    Returns (sentiment_label, reason) or (None, None) if parsing fails.\n",
        "    \"\"\"\n",
        "    # Try to extract from [Sentiment: <label>, Reason: <explanation>] format\n",
        "    pattern = r'\\[Sentiment:\\s*([^,]+),\\s*Reason:\\s*([^\\]]+)\\]'\n",
        "    match = re.search(pattern, output_text, re.IGNORECASE)\n",
        "    \n",
        "    if match:\n",
        "        sentiment = match.group(1).strip()\n",
        "        reason = match.group(2).strip()\n",
        "        return sentiment, reason\n",
        "    \n",
        "    # Fallback: try to find sentiment keywords\n",
        "    sentiment_keywords = {\n",
        "        'positive': 'Positive',\n",
        "        'negative': 'Negative',\n",
        "        'neutral': 'Neutral'\n",
        "    }\n",
        "    \n",
        "    output_lower = output_text.lower()\n",
        "    for keyword, label in sentiment_keywords.items():\n",
        "        if keyword in output_lower:\n",
        "            # Try to extract reason after sentiment\n",
        "            reason_start = output_lower.find(keyword) + len(keyword)\n",
        "            reason = output_text[reason_start:].strip()\n",
        "            if not reason:\n",
        "                reason = \"No reason provided\"\n",
        "            return label, reason\n",
        "    \n",
        "    return None, output_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_with_vicuna(sentence: str, few_shot_examples: str = None, few_shot: bool = False) -> tuple:\n",
        "    \"\"\"\n",
        "    Classify sentiment for a single sentence using Vicuna.\n",
        "    Returns (raw_output, sentiment_label, reason)\n",
        "    \"\"\"\n",
        "    if few_shot and few_shot_examples:\n",
        "        examples_section = FEW_SHOT_EXAMPLES_TEMPLATE.format(examples=few_shot_examples)\n",
        "        prompt = FEW_SHOT_PROMPT.format(examples=examples_section, sentence=sentence)\n",
        "    else:\n",
        "        prompt = ZERO_SHOT_PROMPT.format(sentence=sentence)\n",
        "\n",
        "    # Clear CUDA cache before inference to prevent memory issues\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Use torch.no_grad() for inference to save memory\n",
        "    with torch.no_grad():\n",
        "        outputs = gen_pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            return_full_text=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # Clear cache after inference to free memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # outputs is a list of dicts, we take the first\n",
        "    raw_output = outputs[0][\"generated_text\"].strip()\n",
        "    sentiment_label, reason = parse_sentiment_output(raw_output)\n",
        "    \n",
        "    return raw_output, sentiment_label, reason\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Zero-shot (All Samples - No Splitting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_all_samples_zero_shot(xlsx_path: str):\n",
        "    \"\"\"\n",
        "    Process ALL samples for zero-shot sentiment analysis without splitting.\n",
        "    This uses every row in the dataset as a test sample.\n",
        "    \n",
        "    Args:\n",
        "        xlsx_path: Path to the XLSX file\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with results for all samples\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = load_data_from_xlsx(xlsx_path, sheet_index=1)\n",
        "    \n",
        "    logging.info(f\"Processing all {len(df)} samples for zero-shot analysis (no splitting)\")\n",
        "    \n",
        "    # Prepare results list\n",
        "    results = []\n",
        "    \n",
        "    # Process each sample\n",
        "    for idx, (_, row) in enumerate(df.iterrows(), 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing sentence {idx}/{len(df)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Get column names\n",
        "        machine_col = None\n",
        "        human_col = None\n",
        "        \n",
        "        for col in row.index:\n",
        "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
        "                machine_col = col\n",
        "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
        "                human_col = col\n",
        "        \n",
        "        if machine_col is None or human_col is None:\n",
        "            # Fallback to first two columns\n",
        "            cols = list(row.index)\n",
        "            machine_col = cols[0] if machine_col is None else machine_col\n",
        "            human_col = cols[1] if human_col is None else human_col\n",
        "        \n",
        "        ground_truth = row.get(machine_col, \"\")\n",
        "        hypothesis = row.get(human_col, \"\")\n",
        "        \n",
        "        print(f\"\\nGround Truth: {ground_truth}\")\n",
        "        print(f\"Hypothesis: {hypothesis}\")\n",
        "        \n",
        "        # Build the zero-shot prompt\n",
        "        full_prompt = ZERO_SHOT_PROMPT.format(sentence=hypothesis)\n",
        "        \n",
        "        print(f\"\\nFull Input Prompt:\\n{full_prompt}\")\n",
        "        \n",
        "        # Classify\n",
        "        try:\n",
        "            raw_output, sentiment_label, reason = classify_with_vicuna(\n",
        "                hypothesis, \n",
        "                few_shot_examples=None,\n",
        "                few_shot=False\n",
        "            )\n",
        "            \n",
        "            print(f\"\\nModel Output:\\n{raw_output}\")\n",
        "            print(f\"\\nParsed Sentiment: {sentiment_label}\")\n",
        "            print(f\"Parsed Reason: {reason}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: {e}\")\n",
        "            raw_output = f\"ERROR: {e}\"\n",
        "            sentiment_label = None\n",
        "            reason = None\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'ground_truth': ground_truth,\n",
        "            'hypothesis': hypothesis,\n",
        "            'sentiment_label': sentiment_label if sentiment_label else \"ERROR\",\n",
        "            'reason': reason if reason else raw_output\n",
        "        })\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save to CSV\n",
        "    today = datetime.now().strftime(\"%Y%m%d\")\n",
        "    output_filename = f\"vicuna_13b_v1.3_zero_shot_all_samples_{today}.csv\"\n",
        "    \n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = r\"data/model_outputs/vicuna_13b_v1.3\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Results saved to: {output_path}\")\n",
        "    print(f\"Total samples processed: {len(results_df)}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_sentiment_analysis(xlsx_path: str, few_shot: bool = False, random_seed: int = 42):\n",
        "    \"\"\"\n",
        "    Main workflow:\n",
        "    1. Load data from second sheet of XLSX\n",
        "    2. Split into test and few-shot sets (50/50)\n",
        "    3. Process each test sentence with 5 random examples\n",
        "    4. Save results to CSV\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = load_data_from_xlsx(xlsx_path, sheet_index=1)\n",
        "    \n",
        "    # Split data\n",
        "    test_df, few_shot_df = split_data_into_sets(df, test_ratio=0.5, random_seed=random_seed)\n",
        "    \n",
        "    # Prepare results list\n",
        "    results = []\n",
        "    \n",
        "    # Process each test sentence\n",
        "    for idx, (_, row) in enumerate(test_df.iterrows(), 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing sentence {idx}/{len(test_df)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Get column names\n",
        "        machine_col = None\n",
        "        human_col = None\n",
        "        \n",
        "        for col in row.index:\n",
        "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
        "                machine_col = col\n",
        "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
        "                human_col = col\n",
        "        \n",
        "        if machine_col is None or human_col is None:\n",
        "            # Fallback to first two columns\n",
        "            cols = list(row.index)\n",
        "            machine_col = cols[0] if machine_col is None else machine_col\n",
        "            human_col = cols[1] if human_col is None else human_col\n",
        "        \n",
        "        ground_truth = row.get(machine_col, \"\")\n",
        "        hypothesis = row.get(human_col, \"\")\n",
        "        \n",
        "        print(f\"\\nGround Truth: {ground_truth}\")\n",
        "        print(f\"Hypothesis: {hypothesis}\")\n",
        "        \n",
        "        # Get few-shot examples if needed\n",
        "        few_shot_examples = None\n",
        "        if few_shot:\n",
        "            few_shot_examples = format_few_shot_examples(few_shot_df, num_examples=5)\n",
        "            print(f\"\\nFew-shot Examples:\\n{few_shot_examples}\")\n",
        "        \n",
        "        # Build the prompt to print it\n",
        "        if few_shot and few_shot_examples:\n",
        "            examples_section = FEW_SHOT_EXAMPLES_TEMPLATE.format(examples=few_shot_examples)\n",
        "            full_prompt = FEW_SHOT_PROMPT.format(examples=examples_section, sentence=hypothesis)\n",
        "        else:\n",
        "            full_prompt = ZERO_SHOT_PROMPT.format(sentence=hypothesis)\n",
        "        \n",
        "        print(f\"\\nFull Input Prompt:\\n{full_prompt}\")\n",
        "        \n",
        "        # Classify\n",
        "        try:\n",
        "            raw_output, sentiment_label, reason = classify_with_vicuna(\n",
        "                hypothesis, \n",
        "                few_shot_examples=few_shot_examples,\n",
        "                few_shot=few_shot\n",
        "            )\n",
        "            \n",
        "            print(f\"\\nModel Output:\\n{raw_output}\")\n",
        "            print(f\"\\nParsed Sentiment: {sentiment_label}\")\n",
        "            print(f\"Parsed Reason: {reason}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: {e}\")\n",
        "            raw_output = f\"ERROR: {e}\"\n",
        "            sentiment_label = None\n",
        "            reason = None\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'ground_truth': ground_truth,\n",
        "            'hypothesis': hypothesis,\n",
        "            'sentiment_label': sentiment_label if sentiment_label else \"ERROR\",\n",
        "            'reason': reason if reason else raw_output\n",
        "        })\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save to CSV\n",
        "    today = datetime.now().strftime(\"%Y%m%d\")\n",
        "    shot_type = \"few_shot\" if few_shot else \"zero_shot\"\n",
        "    output_filename = f\"vicuna_13b_v1.3_{shot_type}_{today}.csv\"\n",
        "    \n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = r\"data/model_outputs/vicuna_13b_v1.3\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Results saved to: {output_path}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the path to your XLSX file\n",
        "xlsx_file_path = r\"C:\\Users\\pryce\\OneDrive\\Desktop\\Lost in Transcription\\Text Inputs\\Samples.xlsx\"  # Update this path\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 33\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Zero-shot (Split)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run zero-shot sentiment analysis (50/50 split)\n",
        "zero_shot_results = process_sentiment_analysis(\n",
        "    xlsx_file_path, \n",
        "    few_shot=False,\n",
        "    random_seed=RANDOM_SEED\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Zero-shot (All Samples - No Split)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run zero-shot sentiment analysis on ALL samples (no splitting)\n",
        "all_samples_zero_shot_results = process_all_samples_zero_shot(xlsx_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Few-shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run few-shot sentiment analysis\n",
        "few_shot_results = process_sentiment_analysis(\n",
        "    xlsx_file_path, \n",
        "    few_shot=True,\n",
        "    random_seed=RANDOM_SEED\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
