{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Detection with XLM-RoBERTa (HF, transformers) over WER-binned sample sentences\n",
        "\n",
        "**⚠️ Note:** This model is for **LANGUAGE DETECTION**, not sentiment analysis!\n",
        "\n",
        "XLM-RoBERTa fine-tuned to detect 20 languages with 99.6% accuracy. Useful for:\n",
        "- Identifying language of transcriptions\n",
        "- Preprocessing before language-specific sentiment analysis\n",
        "- Detecting code-switching or multilingual content\n",
        "\n",
        "**Model Details:**\n",
        "- Model: [papluca/xlm-roberta-base-language-detection](https://huggingface.co/papluca/xlm-roberta-base-language-detection)\n",
        "- Base: XLM-RoBERTa-base (~278M parameters)\n",
        "- Accuracy: 99.6% on language identification\n",
        "- License: MIT\n",
        "- Supported Languages: ar, bg, de, el, en, es, fr, hi, it, ja, nl, pl, pt, ru, sw, th, tr, ur, vi, zh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "import logging\n",
        "import random\n",
        "from datetime import datetime\n",
        "import re\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Model Configuration\n",
        "\n",
        "XLM-RoBERTa fine-tuned for language detection across 20 languages.\n",
        "This is a direct classification model (not zero-shot) - very fast and accurate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XLM-RoBERTa fine-tuned for language detection\n",
        "model_id = \"papluca/xlm-roberta-base-language-detection\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "# Check GPU memory (if available)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize the text classification pipeline for language detection\n",
        "lang_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Language code to full name mapping\n",
        "LANGUAGE_NAMES = {\n",
        "    'ar': 'Arabic', 'bg': 'Bulgarian', 'de': 'German', 'el': 'Greek',\n",
        "    'en': 'English', 'es': 'Spanish', 'fr': 'French', 'hi': 'Hindi',\n",
        "    'it': 'Italian', 'ja': 'Japanese', 'nl': 'Dutch', 'pl': 'Polish',\n",
        "    'pt': 'Portuguese', 'ru': 'Russian', 'sw': 'Swahili', 'th': 'Thai',\n",
        "    'tr': 'Turkish', 'ur': 'Urdu', 'vi': 'Vietnamese', 'zh': 'Chinese'\n",
        "}\n",
        "\n",
        "print(f\"✓ XLM-RoBERTa Language Detection model loaded successfully\")\n",
        "print(f\"  Model: {model_id}\")\n",
        "print(f\"  Parameters: ~278M\")\n",
        "print(f\"  Accuracy: 99.6%\")\n",
        "print(f\"  Supported languages: {len(LANGUAGE_NAMES)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Test Pipeline (Optional - Run this to verify setup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test to verify the pipeline works\n",
        "print(\"Testing XLM-RoBERTa language detection pipeline...\")\n",
        "\n",
        "test_sentences = [\n",
        "    \"Hello, how are you today?\",  # English\n",
        "    \"Bonjour, comment allez-vous?\",  # French\n",
        "    \"Hola, ¿cómo estás?\",  # Spanish\n",
        "    \"こんにちは、元気ですか？\",  # Japanese\n",
        "    \"مرحبا، كيف حالك؟\",  # Arabic\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    result = lang_pipeline(sentence, top_k=1, truncation=True)[0]\n",
        "    lang_code = result['label']\n",
        "    lang_name = LANGUAGE_NAMES.get(lang_code, lang_code)\n",
        "    score = result['score']\n",
        "    print(f\"  '{sentence[:30]}...' -> {lang_name} ({lang_code}) [{score:.4f}]\")\n",
        "\n",
        "print(\"\\n✓ Test successful! Language detection is working correctly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading and Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Load Data from XLSX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_xlsx(xlsx_path: str, sheet_index: int = 1) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load data from the second sheet (index 1) of an XLSX file.\n",
        "    Expected columns:\n",
        "    - Machine Transcription (ground truth)\n",
        "    - Human Transcription (hypothesis)\n",
        "    - Sentiment Label\n",
        "    - WER\n",
        "    - CER\n",
        "    - List of AAE features\n",
        "    - List of transcription errors\n",
        "    - Interview\n",
        "    \"\"\"\n",
        "    xl_file = pd.ExcelFile(xlsx_path)\n",
        "    sheet_names = xl_file.sheet_names\n",
        "    if len(sheet_names) <= sheet_index:\n",
        "        raise ValueError(f\"Sheet index {sheet_index} not available. Available sheets: {sheet_names}\")\n",
        "    \n",
        "    df = pd.read_excel(xlsx_path, sheet_name=sheet_index)\n",
        "    logging.info(f\"Loaded {len(df)} rows from sheet '{sheet_names[sheet_index]}'\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data_into_sets(df: pd.DataFrame, test_ratio: float = 0.5, random_seed: int = None) -> tuple:\n",
        "    \"\"\"\n",
        "    Split dataframe into test and example sets.\n",
        "    Returns (test_df, example_df) with equal sizes (or as close as possible).\n",
        "    \n",
        "    This function is kept for API consistency with other model notebooks.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "    \n",
        "    # Shuffle the dataframe\n",
        "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
        "    \n",
        "    # Split in half\n",
        "    split_idx = len(df_shuffled) // 2\n",
        "    test_df = df_shuffled.iloc[:split_idx].copy()\n",
        "    example_df = df_shuffled.iloc[split_idx:].copy()\n",
        "    \n",
        "    logging.info(f\"Split data: {len(test_df)} test samples, {len(example_df)} example samples\")\n",
        "    return test_df, example_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detect Language with Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Language Detection Function\n",
        "\n",
        "This model performs direct classification into one of 20 languages.\n",
        "It's very fast (similar to DistilBERT sentiment) as it's a fine-tuned classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_language(sentence: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Detect the language of a single sentence using XLM-RoBERTa.\n",
        "    \n",
        "    Returns:\n",
        "        (raw_output, language_label, reason)\n",
        "        - raw_output: String representation of detection results\n",
        "        - language_label: Detected language (full name)\n",
        "        - reason: Explanation with confidence score\n",
        "    \"\"\"\n",
        "    # Run language detection\n",
        "    with torch.no_grad():\n",
        "        result = lang_pipeline(sentence, top_k=3, truncation=True)\n",
        "    \n",
        "    # Get top result\n",
        "    top_result = result[0]\n",
        "    lang_code = top_result['label']\n",
        "    lang_name = LANGUAGE_NAMES.get(lang_code, lang_code)\n",
        "    score = top_result['score']\n",
        "    \n",
        "    # Build reason with top-3 predictions\n",
        "    top3_details = \", \".join([\n",
        "        f\"{LANGUAGE_NAMES.get(r['label'], r['label'])}: {r['score']:.2%}\" \n",
        "        for r in result[:3]\n",
        "    ])\n",
        "    reason = f\"Top predictions: {top3_details}\"\n",
        "    \n",
        "    # Format raw output for consistency with other models\n",
        "    raw_output = f\"[Language: {lang_name} ({lang_code}), Score: {score:.4f}]\"\n",
        "    \n",
        "    return raw_output, lang_name, reason\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Process All Samples\n",
        "\n",
        "Detect the language of all transcriptions in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_all_samples(xlsx_path: str):\n",
        "    \"\"\"\n",
        "    Process ALL samples for language detection.\n",
        "    \n",
        "    Args:\n",
        "        xlsx_path: Path to the XLSX file\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with results for all samples\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = load_data_from_xlsx(xlsx_path, sheet_index=1)\n",
        "    \n",
        "    logging.info(f\"Processing all {len(df)} samples for language detection\")\n",
        "    \n",
        "    # Prepare results list\n",
        "    results = []\n",
        "    \n",
        "    # Process each sample\n",
        "    for idx, (_, row) in enumerate(df.iterrows(), 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing sentence {idx}/{len(df)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Get column names\n",
        "        machine_col = None\n",
        "        human_col = None\n",
        "        \n",
        "        for col in row.index:\n",
        "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
        "                machine_col = col\n",
        "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
        "                human_col = col\n",
        "        \n",
        "        if machine_col is None or human_col is None:\n",
        "            # Fallback to first two columns\n",
        "            cols = list(row.index)\n",
        "            machine_col = cols[0] if machine_col is None else machine_col\n",
        "            human_col = cols[1] if human_col is None else human_col\n",
        "        \n",
        "        ground_truth = row.get(machine_col, \"\")\n",
        "        hypothesis = row.get(human_col, \"\")\n",
        "        \n",
        "        print(f\"\\nGround Truth: {ground_truth}\")\n",
        "        print(f\"Hypothesis: {hypothesis}\")\n",
        "        \n",
        "        # Detect language\n",
        "        try:\n",
        "            raw_output, language_label, reason = detect_language(hypothesis)\n",
        "            \n",
        "            print(f\"\\nModel Output: {raw_output}\")\n",
        "            print(f\"Detected Language: {language_label}\")\n",
        "            print(f\"Reason: {reason}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: {e}\")\n",
        "            raw_output = f\"ERROR: {e}\"\n",
        "            language_label = \"ERROR\"\n",
        "            reason = str(e)\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'ground_truth': ground_truth,\n",
        "            'hypothesis': hypothesis,\n",
        "            'detected_language': language_label,\n",
        "            'reason': reason\n",
        "        })\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save to CSV\n",
        "    today = datetime.now().strftime(\"%Y%m%d\")\n",
        "    output_filename = f\"xlm_roberta_language_detection_all_samples_{today}.csv\"\n",
        "    \n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = r\"data/model_outputs/xlm_roberta_language_detection\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Results saved to: {output_path}\")\n",
        "    print(f\"Total samples processed: {len(results_df)}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Print language distribution\n",
        "    print(\"\\nLanguage Distribution:\")\n",
        "    lang_counts = results_df['detected_language'].value_counts()\n",
        "    for lang, count in lang_counts.items():\n",
        "        print(f\"  {lang}: {count} ({count/len(results_df):.1%})\")\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_language_detection(xlsx_path: str, random_seed: int = 42):\n",
        "    \"\"\"\n",
        "    Process language detection with data splitting.\n",
        "    \n",
        "    Args:\n",
        "        xlsx_path: Path to the XLSX file\n",
        "        random_seed: Random seed for data splitting\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = load_data_from_xlsx(xlsx_path, sheet_index=1)\n",
        "    \n",
        "    # Split data (for consistency with other notebooks)\n",
        "    test_df, _ = split_data_into_sets(df, test_ratio=0.5, random_seed=random_seed)\n",
        "    \n",
        "    logging.info(f\"Processing {len(test_df)} samples for language detection (split mode)\")\n",
        "    \n",
        "    # Prepare results list\n",
        "    results = []\n",
        "    \n",
        "    # Process each test sentence\n",
        "    for idx, (_, row) in enumerate(test_df.iterrows(), 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing sentence {idx}/{len(test_df)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Get column names\n",
        "        machine_col = None\n",
        "        human_col = None\n",
        "        \n",
        "        for col in row.index:\n",
        "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
        "                machine_col = col\n",
        "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
        "                human_col = col\n",
        "        \n",
        "        if machine_col is None or human_col is None:\n",
        "            cols = list(row.index)\n",
        "            machine_col = cols[0] if machine_col is None else machine_col\n",
        "            human_col = cols[1] if human_col is None else human_col\n",
        "        \n",
        "        ground_truth = row.get(machine_col, \"\")\n",
        "        hypothesis = row.get(human_col, \"\")\n",
        "        \n",
        "        print(f\"\\nGround Truth: {ground_truth}\")\n",
        "        print(f\"Hypothesis: {hypothesis}\")\n",
        "        \n",
        "        # Detect language\n",
        "        try:\n",
        "            raw_output, language_label, reason = detect_language(hypothesis)\n",
        "            \n",
        "            print(f\"\\nModel Output: {raw_output}\")\n",
        "            print(f\"Detected Language: {language_label}\")\n",
        "            print(f\"Reason: {reason}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: {e}\")\n",
        "            raw_output = f\"ERROR: {e}\"\n",
        "            language_label = \"ERROR\"\n",
        "            reason = str(e)\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'ground_truth': ground_truth,\n",
        "            'hypothesis': hypothesis,\n",
        "            'detected_language': language_label,\n",
        "            'reason': reason\n",
        "        })\n",
        "    \n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save to CSV\n",
        "    today = datetime.now().strftime(\"%Y%m%d\")\n",
        "    output_filename = f\"xlm_roberta_language_detection_split_{today}.csv\"\n",
        "    \n",
        "    output_dir = r\"data/model_outputs/xlm_roberta_language_detection\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Results saved to: {output_path}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Processing (Faster)\n",
        "\n",
        "Language detection is very fast - batch processing provides significant speedup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_all_samples_batch(xlsx_path: str, batch_size: int = 32):\n",
        "    \"\"\"\n",
        "    Process ALL samples using batch processing for faster inference.\n",
        "    \n",
        "    Language detection is very fast, so larger batch sizes work well.\n",
        "    \n",
        "    Args:\n",
        "        xlsx_path: Path to the XLSX file\n",
        "        batch_size: Number of sentences to process at once\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with results for all samples\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = load_data_from_xlsx(xlsx_path, sheet_index=1)\n",
        "    \n",
        "    logging.info(f\"Processing all {len(df)} samples (batch mode, batch_size={batch_size})\")\n",
        "    \n",
        "    # Get column names from first row\n",
        "    sample_row = df.iloc[0]\n",
        "    machine_col = None\n",
        "    human_col = None\n",
        "    \n",
        "    for col in sample_row.index:\n",
        "        if 'machine' in col.lower() and 'transcription' in col.lower():\n",
        "            machine_col = col\n",
        "        elif 'human' in col.lower() and 'transcription' in col.lower():\n",
        "            human_col = col\n",
        "    \n",
        "    if machine_col is None or human_col is None:\n",
        "        cols = list(sample_row.index)\n",
        "        machine_col = cols[0] if machine_col is None else machine_col\n",
        "        human_col = cols[1] if human_col is None else human_col\n",
        "    \n",
        "    # Extract sentences\n",
        "    ground_truths = df[machine_col].tolist()\n",
        "    hypotheses = df[human_col].tolist()\n",
        "    \n",
        "    # Process in batches\n",
        "    print(f\"Processing {len(hypotheses)} sentences in batches of {batch_size}...\")\n",
        "    \n",
        "    all_results = []\n",
        "    for i in range(0, len(hypotheses), batch_size):\n",
        "        batch = hypotheses[i:i+batch_size]\n",
        "        # Convert any non-string to string\n",
        "        batch = [str(s) if not isinstance(s, str) else s for s in batch]\n",
        "        batch_results = lang_pipeline(batch, top_k=3, truncation=True)\n",
        "        all_results.extend(batch_results)\n",
        "        print(f\"  Processed {min(i+batch_size, len(hypotheses))}/{len(hypotheses)} sentences\")\n",
        "    \n",
        "    # Build results dataframe\n",
        "    results = []\n",
        "    for idx, (gt, hyp, res) in enumerate(zip(ground_truths, hypotheses, all_results)):\n",
        "        top_result = res[0]\n",
        "        lang_code = top_result['label']\n",
        "        lang_name = LANGUAGE_NAMES.get(lang_code, lang_code)\n",
        "        score = top_result['score']\n",
        "        \n",
        "        top3_details = \", \".join([\n",
        "            f\"{LANGUAGE_NAMES.get(r['label'], r['label'])}: {r['score']:.2%}\" \n",
        "            for r in res[:3]\n",
        "        ])\n",
        "        reason = f\"Top predictions: {top3_details}\"\n",
        "        \n",
        "        results.append({\n",
        "            'ground_truth': gt,\n",
        "            'hypothesis': hyp,\n",
        "            'detected_language': lang_name,\n",
        "            'reason': reason\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save to CSV\n",
        "    today = datetime.now().strftime(\"%Y%m%d\")\n",
        "    output_filename = f\"xlm_roberta_language_detection_batch_all_samples_{today}.csv\"\n",
        "    \n",
        "    output_dir = r\"data/model_outputs/xlm_roberta_language_detection\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Results saved to: {output_path}\")\n",
        "    print(f\"Total samples processed: {len(results_df)}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Print language distribution\n",
        "    print(\"\\nLanguage Distribution:\")\n",
        "    lang_counts = results_df['detected_language'].value_counts()\n",
        "    for lang, count in lang_counts.items():\n",
        "        print(f\"  {lang}: {count} ({count/len(results_df):.1%})\")\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the path to your XLSX file\n",
        "xlsx_file_path = r\"C:\\Users\\pryce\\OneDrive\\Desktop\\Lost in Transcription\\Text Inputs\\Samples.xlsx\"  # Update this path\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 33\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Option 1: Process with Split (50% of data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run language detection on 50% of data (split mode)\n",
        "split_results = process_language_detection(\n",
        "    xlsx_file_path, \n",
        "    random_seed=RANDOM_SEED\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Option 2: Process All Samples (One by One)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run language detection on ALL samples (verbose, one by one)\n",
        "all_results = process_all_samples(xlsx_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Option 3: Batch Processing (Recommended - Fastest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run language detection on ALL samples using batch processing (fastest)\n",
        "# Language detection is fast, so batch_size=32 works well\n",
        "batch_results = process_all_samples_batch(xlsx_file_path, batch_size=32)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
