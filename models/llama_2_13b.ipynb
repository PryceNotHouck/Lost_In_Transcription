{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e590d6b77bb125a8",
   "metadata": {},
   "source": [
    "# Sentiment Classification tasks with Llama-2-13B (HF, transformers) over WER-binned sample sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7189eac44d6f2d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1041afc14a75664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:14:47.701076Z",
     "start_time": "2025-11-26T00:14:36.473263Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "import re\n",
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfacffe",
   "metadata": {},
   "source": [
    "###### Memory Management Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b48ba1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:14:51.761381Z",
     "start_time": "2025-11-26T00:14:51.698374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable to help with memory fragmentation\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear any existing CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748469913234fd69",
   "metadata": {},
   "source": [
    "###### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348f3c42ea3ec1a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:03:05.977370Z",
     "start_time": "2025-11-26T00:01:50.611439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d12f36793f40f6966a9f9fe2e80f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAcceleratorError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     14\u001B[39m model = AutoModelForCausalLM.from_pretrained(\n\u001B[32m     15\u001B[39m     model_id,\n\u001B[32m     16\u001B[39m     \u001B[38;5;66;03m#device_map = \"auto\",\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     19\u001B[39m     \u001B[38;5;66;03m# TODO: consider quantization settings based on hardware\u001B[39;00m\n\u001B[32m     20\u001B[39m )\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Initialize the generation pipeline\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# Don't use device_map in pipeline when model already has device_map=\"auto\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m gen_pipeline = \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtext-generation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat16\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_available\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat32\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_available\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\n\u001B[32m     30\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1230\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m   1227\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m processor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1228\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mprocessor\u001B[39m\u001B[33m\"\u001B[39m] = processor\n\u001B[32m-> \u001B[39m\u001B[32m1230\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpipeline_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframework\u001B[49m\u001B[43m=\u001B[49m\u001B[43mframework\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:121\u001B[39m, in \u001B[36mTextGenerationPipeline.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m     \u001B[38;5;28mself\u001B[39m.check_model_type(\n\u001B[32m    123\u001B[39m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mtf\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001B[32m    124\u001B[39m     )\n\u001B[32m    125\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mprefix\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._preprocess_params:\n\u001B[32m    126\u001B[39m         \u001B[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001B[39;00m\n\u001B[32m    127\u001B[39m         \u001B[38;5;66;03m# as a \"default\".\u001B[39;00m\n\u001B[32m    128\u001B[39m         \u001B[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001B[39;00m\n\u001B[32m    129\u001B[39m         \u001B[38;5;66;03m# which is why we cannot put them in their respective methods.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\base.py:1044\u001B[39m, in \u001B[36mPipeline.__init__\u001B[39m\u001B[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001B[39m\n\u001B[32m   1037\u001B[39m \u001B[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001B[39;00m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1039\u001B[39m     \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1040\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.device != \u001B[38;5;28mself\u001B[39m.device\n\u001B[32m   1041\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.device, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device < \u001B[32m0\u001B[39m)\n\u001B[32m   1042\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m hf_device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1043\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1044\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001B[39;00m\n\u001B[32m   1047\u001B[39m \u001B[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001B[39;00m\n\u001B[32m   1048\u001B[39m \u001B[38;5;66;03m# tweaks to the generation config.\u001B[39;00m\n\u001B[32m   1049\u001B[39m \u001B[38;5;66;03m# 2 - load the assistant model if it is passed.\u001B[39;00m\n\u001B[32m   1050\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pipeline_calls_generate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.can_generate():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\modeling_utils.py:4462\u001B[39m, in \u001B[36mPreTrainedModel.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   4457\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[32m   4458\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   4459\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4460\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m `dtype` by passing the correct `dtype` argument.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4461\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m4462\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1369\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1366\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1367\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1369\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 928 (2 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:955\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    951\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    952\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m955\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    956\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    958\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1348\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1349\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1350\u001B[39m             device,\n\u001B[32m   1351\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1352\u001B[39m             non_blocking,\n\u001B[32m   1353\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1354\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1355\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1356\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1357\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1361\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mAcceleratorError\u001B[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = False, trust_remote_code = True)\n",
    "\n",
    "# Set pad_token for Llama models (they don't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    # TODO: consider quantization settings based on hardware\n",
    ")\n",
    "\n",
    "# Initialize the generation pipeline\n",
    "# Don't use device_map in pipeline when model already has device_map=\"auto\"\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5be95e3dbe59af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:15:37.765464Z",
     "start_time": "2025-11-26T00:14:55.259961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Total GPU Memory: 4.00 GB\n",
      "Loading model with 8-bit quantization (reduces memory from ~26GB to ~7GB)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pryce\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\utils\\modeling.py:913: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  _ = torch.tensor([0], device=i)\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c01eb01ec454cb249569b713f42e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with 8-bit quantization\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = False, trust_remote_code = True)\n",
    "\n",
    "# Set pad_token for Llama models (they don't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Try to use 8-bit quantization to reduce memory usage from ~26GB to ~7GB\n",
    "# This is essential for GPUs with limited memory\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    # Configure 8-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    print(\"Loading model with 8-bit quantization (reduces memory from ~26GB to ~7GB)...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",  # Automatically offload to CPU if needed\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    print(\"✓ Model loaded with 8-bit quantization\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⚠ bitsandbytes not available. Please install: pip install bitsandbytes\")\n",
    "    print(\"Falling back to CPU offloading with float16...\")\n",
    "\n",
    "    # Fallback: Use device_map=\"auto\" to offload layers to CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        max_memory={0: \"4GiB\", \"cpu\": \"30GiB\"} if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    print(\"✓ Model loaded with CPU offloading\")\n",
    "\n",
    "# Initialize the generation pipeline\n",
    "# When using device_map=\"auto\", the pipeline handles device placement automatically\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d34b4",
   "metadata": {},
   "source": [
    "###### Test Pipeline (Optional - Run this to verify setup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629e5f7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:18.339204Z",
     "start_time": "2025-11-26T00:12:12.479968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with a simple prompt...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotImplementedError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTesting pipeline with a simple prompt...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m test_prompt = \u001B[33m\"\u001B[39m\u001B[33mHello, how are you?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m test_output = \u001B[43mgen_pipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtest_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_full_text\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTest successful! Output: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_output[\u001B[32m0\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mgenerated_text\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mIf this test hangs or takes more than 30 seconds, there\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms a configuration issue.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001B[39m, in \u001B[36mTextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, text_inputs, **kwargs)\u001B[39m\n\u001B[32m    330\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\base.py:1467\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1459\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1460\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1461\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1464\u001B[39m         )\n\u001B[32m   1465\u001B[39m     )\n\u001B[32m   1466\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1467\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\base.py:1474\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1472\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1473\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1474\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1475\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1476\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1372\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1373\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1374\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1375\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001B[39m, in \u001B[36mTextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    430\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m432\u001B[39m output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, ModelOutput):\n\u001B[32m    435\u001B[39m     generated_sequence = output.sequences\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\generation\\utils.py:2539\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2528\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m GenerationMixin.generate(\n\u001B[32m   2529\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   2530\u001B[39m         inputs,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2534\u001B[39m         **kwargs,\n\u001B[32m   2535\u001B[39m     )\n\u001B[32m   2537\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001B[32m   2538\u001B[39m     \u001B[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2539\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2540\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2541\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2542\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2543\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2544\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2545\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2546\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2547\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2549\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2550\u001B[39m     \u001B[38;5;66;03m# 11. run beam sample\u001B[39;00m\n\u001B[32m   2551\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._beam_search(\n\u001B[32m   2552\u001B[39m         input_ids,\n\u001B[32m   2553\u001B[39m         logits_processor=prepared_logits_processor,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2557\u001B[39m         **model_kwargs,\n\u001B[32m   2558\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\generation\\utils.py:2867\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   2864\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_hidden_states\u001B[39m\u001B[33m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m   2866\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[32m-> \u001B[39m\u001B[32m2867\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   2868\u001B[39m     is_prefill = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   2869\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\hooks.py:170\u001B[39m, in \u001B[36madd_hook_to_module.<locals>.new_forward\u001B[39m\u001B[34m(module, *args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m         output = module._old_forward(*args, **kwargs)\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     output = \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\utils\\generic.py:940\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    939\u001B[39m     return_dict = return_dict_passed\n\u001B[32m--> \u001B[39m\u001B[32m940\u001B[39m output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    942\u001B[39m     output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001B[39m, in \u001B[36mLlamaForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    427\u001B[39m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[32m    428\u001B[39m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m    440\u001B[39m     **kwargs: Unpack[TransformersKwargs],\n\u001B[32m    441\u001B[39m ) -> CausalLMOutputWithPast:\n\u001B[32m    442\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    443\u001B[39m \u001B[33;03m    Example:\u001B[39;00m\n\u001B[32m    444\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    457\u001B[39m \u001B[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001B[39;00m\n\u001B[32m    458\u001B[39m \u001B[33;03m    ```\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m459\u001B[39m     outputs: BaseModelOutputWithPast = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    463\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    464\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    465\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    470\u001B[39m     hidden_states = outputs.last_hidden_state\n\u001B[32m    471\u001B[39m     \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1061\u001B[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001B[32m   1062\u001B[39m                 monkey_patched_layers.append((module, original_forward))\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[38;5;66;03m# Restore original forward methods\u001B[39;00m\n\u001B[32m   1066\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module, original_forward \u001B[38;5;129;01min\u001B[39;00m monkey_patched_layers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001B[39m, in \u001B[36mLlamaModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001B[39m\n\u001B[32m    392\u001B[39m position_embeddings = \u001B[38;5;28mself\u001B[39m.rotary_emb(hidden_states, position_ids)\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m decoder_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers[: \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers]:\n\u001B[32m--> \u001B[39m\u001B[32m395\u001B[39m     hidden_states = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    405\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.norm(hidden_states)\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutputWithPast(\n\u001B[32m    407\u001B[39m     last_hidden_state=hidden_states,\n\u001B[32m    408\u001B[39m     past_key_values=past_key_values,\n\u001B[32m    409\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\hooks.py:170\u001B[39m, in \u001B[36madd_hook_to_module.<locals>.new_forward\u001B[39m\u001B[34m(module, *args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m         output = module._old_forward(*args, **kwargs)\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     output = \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:294\u001B[39m, in \u001B[36mLlamaDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001B[39m\n\u001B[32m    292\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.input_layernorm(hidden_states)\n\u001B[32m    293\u001B[39m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m hidden_states, _ = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    301\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    302\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    303\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m hidden_states = residual + hidden_states\n\u001B[32m    306\u001B[39m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\hooks.py:170\u001B[39m, in \u001B[36madd_hook_to_module.<locals>.new_forward\u001B[39m\u001B[34m(module, *args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m         output = module._old_forward(*args, **kwargs)\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     output = \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:236\u001B[39m, in \u001B[36mLlamaAttention.forward\u001B[39m\u001B[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001B[39m\n\u001B[32m    233\u001B[39m input_shape = hidden_states.shape[:-\u001B[32m1\u001B[39m]\n\u001B[32m    234\u001B[39m hidden_shape = (*input_shape, -\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m.head_dim)\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m query_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mq_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m.view(hidden_shape).transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m    237\u001B[39m key_states = \u001B[38;5;28mself\u001B[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m    238\u001B[39m value_states = \u001B[38;5;28mself\u001B[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\hooks.py:165\u001B[39m, in \u001B[36madd_hook_to_module.<locals>.new_forward\u001B[39m\u001B[34m(module, *args, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mnew_forward\u001B[39m(module, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m165\u001B[39m     args, kwargs = \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_hf_hook\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpre_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m module._hf_hook.no_grad:\n\u001B[32m    167\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\hooks.py:355\u001B[39m, in \u001B[36mAlignDevicesHook.pre_forward\u001B[39m\u001B[34m(self, module, *args, **kwargs)\u001B[39m\n\u001B[32m    347\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    348\u001B[39m             value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    349\u001B[39m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tied_params_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    350\u001B[39m             \u001B[38;5;129;01mand\u001B[39;00m value.data_ptr() \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tied_params_map\n\u001B[32m    351\u001B[39m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.execution_device \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tied_params_map[value.data_ptr()]\n\u001B[32m    352\u001B[39m         ):\n\u001B[32m    353\u001B[39m             \u001B[38;5;28mself\u001B[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001B[38;5;28mself\u001B[39m.execution_device))\n\u001B[32m--> \u001B[39m\u001B[32m355\u001B[39m         \u001B[43mset_module_tensor_to_device\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m            \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m            \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfp16_statistics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfp16_statistics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    364\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m send_to_device(args, \u001B[38;5;28mself\u001B[39m.execution_device), send_to_device(\n\u001B[32m    365\u001B[39m     kwargs, \u001B[38;5;28mself\u001B[39m.execution_device, skip_keys=\u001B[38;5;28mself\u001B[39m.skip_keys\n\u001B[32m    366\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\accelerate\\utils\\modeling.py:436\u001B[39m, in \u001B[36mset_module_tensor_to_device\u001B[39m\u001B[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001B[39m\n\u001B[32m    434\u001B[39m         new_value.SCB = new_value.SCB.to(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    435\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m436\u001B[39m         new_value = \u001B[43mparam_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequires_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mold_value\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequires_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    437\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m param_cls.\u001B[34m__name__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mQTensor\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mQBitsTensor\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    438\u001B[39m     new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad).to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LostInTranscription\\LIT\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:701\u001B[39m, in \u001B[36mInt8Params.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    698\u001B[39m     new_param.CB = new_param.data\n\u001B[32m    700\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.SCB \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m701\u001B[39m         new_param.SCB = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mSCB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    703\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m new_param\n",
      "\u001B[31mNotImplementedError\u001B[39m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "# Quick test to verify the pipeline works\n",
    "# This should complete in a few seconds, not minutes\n",
    "print(\"Testing pipeline with a simple prompt...\")\n",
    "test_prompt = \"Hello, how are you?\"\n",
    "test_output = gen_pipeline(\n",
    "    test_prompt,\n",
    "    max_new_tokens = 20,\n",
    "    do_sample = False,\n",
    "    return_full_text = False,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")\n",
    "print(f\"Test successful! Output: {test_output[0]['generated_text']}\")\n",
    "print(\"\\nIf this test hangs or takes more than 30 seconds, there's a configuration issue.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2a834f6509405",
   "metadata": {},
   "source": [
    "###### Prompt Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4562ccbefdf823ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:34.972656Z",
     "start_time": "2025-11-26T00:12:34.967763Z"
    }
   },
   "outputs": [],
   "source": [
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
    "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
    "2) A short explanation (1–2 sentences) of why you chose that label\n",
    "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
    "4) (Optionally) any caveats or uncertainty if applicable\n",
    "<</SYS>>\n",
    "[INST]\n",
    "User: {sentence}\n",
    "[/INST]\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2386446f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:25:33.875685Z",
     "start_time": "2025-11-25T22:25:33.867732Z"
    }
   },
   "outputs": [],
   "source": [
    "ZERO_SHOT_PROMPT_CONTINUOUS = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
    "1) An integer value for sentiment between -10 and 10, with -10 being the most negative and 10 being the most positive, and a score of 0 being neutral\n",
    "2) A short explanation (1–2 sentences) of why you chose that label\n",
    "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
    "4) (Optionally) any caveats or uncertainty if applicable\n",
    "<</SYS>>\n",
    "[INST]\n",
    "User: {sentence}\n",
    "[/INST]\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d345cdf00d032b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:25:35.096347Z",
     "start_time": "2025-11-25T22:25:35.089715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Format for few-shot examples - will be populated dynamically\n",
    "FEW_SHOT_EXAMPLES_TEMPLATE = \"\"\"\n",
    "### EXAMPLES ###\n",
    "{examples}\n",
    "### END EXAMPLES ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f725010bd8a2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:25:35.955121Z",
     "start_time": "2025-11-25T22:25:35.949729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Few-shot prompt template\n",
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
    "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
    "2) A short explanation (1–2 sentences) of why you chose that label\n",
    "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
    "4) (Optionally) any caveats or uncertainty if applicable\n",
    "<</SYS>>\n",
    "{examples}\n",
    "[INST]\n",
    "User: {sentence}\n",
    "[/INST]\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7717998b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T22:25:37.800643Z",
     "start_time": "2025-11-25T22:25:37.784630Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: stipulate that the model should return in the format of the provided examples\n",
    "FEW_SHOT_PROMPT_CONTINUOUS = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
    "1) An integer value for sentiment between -10 and 10, with -10 being the most negative and 10 being the most positive, and a score of 0 being neutral\n",
    "2) A short explanation (1–2 sentences) of why you chose that label\n",
    "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
    "4) (Optionally) any caveats or uncertainty if applicable\n",
    "<</SYS>>\n",
    "{examples}\n",
    "[INST]\n",
    "User: {sentence}\n",
    "[/INST]\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b05c6c4558c185",
   "metadata": {},
   "source": [
    "### Data Loading and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de2546a0f70c36",
   "metadata": {},
   "source": [
    "###### Load Data from XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512e78a758acca0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:40.262748Z",
     "start_time": "2025-11-26T00:12:40.255951Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_xlsx(xlsx_path: str, sheet_index: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from the second sheet (index 1) of an XLSX file.\n",
    "    Expected columns:\n",
    "    - Machine Transcription (ground truth)\n",
    "    - Human Transcription (hypothesis)\n",
    "    - Sentiment Label\n",
    "    - WER\n",
    "    - CER\n",
    "    - List of AAE features\n",
    "    - List of transcription errors\n",
    "    - Interview\n",
    "    \"\"\"\n",
    "    xl_file = pd.ExcelFile(xlsx_path)\n",
    "    sheet_names = xl_file.sheet_names\n",
    "    if len(sheet_names) <= sheet_index:\n",
    "        raise ValueError(f\"Sheet index {sheet_index} not available. Available sheets: {sheet_names}\")\n",
    "    \n",
    "    df = pd.read_excel(xlsx_path, sheet_name=sheet_index)\n",
    "    logging.info(f\"Loaded {len(df)} rows from sheet '{sheet_names[sheet_index]}'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f18fbb9bf0ce37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:41.724164Z",
     "start_time": "2025-11-26T00:12:41.717285Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_into_sets(df: pd.DataFrame, test_ratio: float = 0.5, random_seed: int = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Split dataframe into test and few-shot example sets.\n",
    "    Returns (test_df, few_shot_df) with equal sizes (or as close as possible).\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    # Shuffle the dataframe\n",
    "    df_shuffled = df.sample(frac = 1, random_state = random_seed).reset_index(drop = True)\n",
    "    \n",
    "    # Split in half\n",
    "    split_idx = len(df_shuffled) // 2\n",
    "    test_df = df_shuffled.iloc[:split_idx].copy()\n",
    "    few_shot_df = df_shuffled.iloc[split_idx:].copy()\n",
    "    \n",
    "    logging.info(f\"Split data: {len(test_df)} test samples, {len(few_shot_df)} few-shot examples\")\n",
    "    return test_df, few_shot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0060a",
   "metadata": {},
   "source": [
    "### Classify with Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878d360",
   "metadata": {},
   "source": [
    "###### Classify with Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deafab88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:46.148217Z",
     "start_time": "2025-11-26T00:12:46.140810Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_few_shot_examples(few_shot_df: pd.DataFrame, num_examples: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Format 5 randomly selected examples from few_shot_df.\n",
    "    Uses Machine Transcription (column 1) for examples as specified.\n",
    "    \"\"\"\n",
    "    if len(few_shot_df) < num_examples:\n",
    "        num_examples = len(few_shot_df)\n",
    "    \n",
    "    # Randomly select examples\n",
    "    selected = few_shot_df.sample(n=num_examples, random_state=None)\n",
    "    \n",
    "    examples_text = []\n",
    "    for idx, (_, row) in enumerate(selected.iterrows(), 1):\n",
    "        # Get column names - handle variations\n",
    "        machine_col = None\n",
    "        human_col = None\n",
    "        sentiment_col = None\n",
    "        \n",
    "        for col in row.index:\n",
    "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
    "                machine_col = col\n",
    "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
    "                human_col = col\n",
    "            elif 'sentiment' in col.lower():\n",
    "                sentiment_col = col\n",
    "        \n",
    "        if machine_col is None or human_col is None:\n",
    "            # Try to find by position or common names\n",
    "            cols = list(row.index)\n",
    "            if len(cols) >= 2:\n",
    "                machine_col = cols[0] if machine_col is None else machine_col\n",
    "                human_col = cols[1] if human_col is None else human_col\n",
    "        \n",
    "        # Use Machine Transcription (column 1) for examples as specified\n",
    "        machine_text = row.get(machine_col, \"N/A\")\n",
    "        sentiment = row.get(sentiment_col, \"N/A\")\n",
    "        \n",
    "        # Format as example for the prompt\n",
    "        example = f\"{idx}. User: {machine_text}\\n   Assistant: [Sentiment: {sentiment}, Reason: This is an example sentence.]\"\n",
    "        examples_text.append(example)\n",
    "    \n",
    "    return \"\\n\".join(examples_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7bc1301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:48.174912Z",
     "start_time": "2025-11-26T00:12:48.167907Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_sentiment_output(output_text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parse sentiment label and reason from model output.\n",
    "    Returns (sentiment_label, reason) or (None, None) if parsing fails.\n",
    "    \"\"\"\n",
    "    # Try to extract from [Sentiment: <label>, Reason: <explanation>] format\n",
    "    pattern = r'\\[Sentiment:\\s*([^,]+),\\s*Reason:\\s*([^\\]]+)\\]'\n",
    "    match = re.search(pattern, output_text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        sentiment = match.group(1).strip()\n",
    "        reason = match.group(2).strip()\n",
    "        return sentiment, reason\n",
    "    \n",
    "    # Fallback: try to find sentiment keywords\n",
    "    sentiment_keywords = {\n",
    "        'positive': 'Positive',\n",
    "        'negative': 'Negative',\n",
    "        'neutral': 'Neutral'\n",
    "    }\n",
    "    \n",
    "    output_lower = output_text.lower()\n",
    "    for keyword, label in sentiment_keywords.items():\n",
    "        if keyword in output_lower:\n",
    "            # Try to extract reason after sentiment\n",
    "            reason_start = output_lower.find(keyword) + len(keyword)\n",
    "            reason = output_text[reason_start:].strip()\n",
    "            if not reason:\n",
    "                reason = \"No reason provided\"\n",
    "            return label, reason\n",
    "    \n",
    "    return None, output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f58bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:50.151978Z",
     "start_time": "2025-11-26T00:12:50.146066Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_with_llama(sentence: str, few_shot_examples: str = None, few_shot: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Classify sentiment for a single sentence.\n",
    "    Returns (raw_output, sentiment_label, reason)\n",
    "    \"\"\"\n",
    "    if few_shot and few_shot_examples:\n",
    "        examples_section = FEW_SHOT_EXAMPLES_TEMPLATE.format(examples = few_shot_examples)\n",
    "        prompt = FEW_SHOT_PROMPT.format(examples = examples_section, sentence = sentence)\n",
    "    else:\n",
    "        prompt = ZERO_SHOT_PROMPT.format(sentence = sentence)\n",
    "\n",
    "    # Use torch.no_grad() for inference to save memory\n",
    "    with torch.no_grad():\n",
    "        outputs = gen_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens = 100,\n",
    "            do_sample = False,\n",
    "            temperature = 0.0,\n",
    "            return_full_text = False,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # outputs is a list of dicts, we take the first\n",
    "    raw_output = outputs[0][\"generated_text\"].strip()\n",
    "    sentiment_label, reason = parse_sentiment_output(raw_output)\n",
    "    \n",
    "    return raw_output, sentiment_label, reason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6e1c7",
   "metadata": {},
   "source": [
    "### Main Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d6fee",
   "metadata": {},
   "source": [
    "###### Zero-shot (All Samples - No Splitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_samples_zero_shot(xlsx_path: str):\n",
    "    \"\"\"\n",
    "    Process ALL samples for zero-shot sentiment analysis without splitting.\n",
    "    This uses every row in the dataset as a test sample.\n",
    "    \n",
    "    Args:\n",
    "        xlsx_path: Path to the XLSX file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results for all samples\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data_from_xlsx(xlsx_path, sheet_index=1)\n",
    "    \n",
    "    logging.info(f\"Processing all {len(df)} samples for zero-shot analysis (no splitting)\")\n",
    "    \n",
    "    # Prepare results list\n",
    "    results = []\n",
    "    \n",
    "    # Process each sample\n",
    "    for idx, (_, row) in enumerate(df.iterrows(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing sentence {idx}/{len(df)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get column names\n",
    "        machine_col = None\n",
    "        human_col = None\n",
    "        \n",
    "        for col in row.index:\n",
    "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
    "                machine_col = col\n",
    "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
    "                human_col = col\n",
    "        \n",
    "        if machine_col is None or human_col is None:\n",
    "            # Fallback to first two columns\n",
    "            cols = list(row.index)\n",
    "            machine_col = cols[0] if machine_col is None else machine_col\n",
    "            human_col = cols[1] if human_col is None else human_col\n",
    "        \n",
    "        ground_truth = row.get(machine_col, \"\")\n",
    "        hypothesis = row.get(human_col, \"\")\n",
    "        \n",
    "        print(f\"\\nGround Truth: {ground_truth}\")\n",
    "        print(f\"Hypothesis: {hypothesis}\")\n",
    "        \n",
    "        # Build the zero-shot prompt\n",
    "        full_prompt = ZERO_SHOT_PROMPT.format(sentence=hypothesis)\n",
    "        \n",
    "        print(f\"\\nFull Input Prompt:\\n{full_prompt}\")\n",
    "        \n",
    "        # Classify\n",
    "        try:\n",
    "            raw_output, sentiment_label, reason = classify_with_llama(\n",
    "                hypothesis, \n",
    "                few_shot_examples=None,\n",
    "                few_shot=False\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nModel Output:\\n{raw_output}\")\n",
    "            print(f\"\\nParsed Sentiment: {sentiment_label}\")\n",
    "            print(f\"Parsed Reason: {reason}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: {e}\")\n",
    "            raw_output = f\"ERROR: {e}\"\n",
    "            sentiment_label = None\n",
    "            reason = None\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'ground_truth': ground_truth,\n",
    "            'hypothesis': hypothesis,\n",
    "            'sentiment_label': sentiment_label if sentiment_label else \"ERROR\",\n",
    "            'reason': reason if reason else raw_output\n",
    "        })\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    output_filename = f\"llama_2_13b_zero_shot_all_samples_{today}.csv\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = r\"data/model_outputs/llama_2_13b\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    print(f\"Total samples processed: {len(results_df)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "254b3c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:53.326010Z",
     "start_time": "2025-11-26T00:12:53.314707Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_sentiment_analysis(xlsx_path: str, few_shot: bool = False, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Main workflow:\n",
    "    1. Load data from second sheet of XLSX\n",
    "    2. Split into test and few-shot sets (50/50)\n",
    "    3. Process each test sentence with 5 random examples\n",
    "    4. Save results to CSV\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data_from_xlsx(xlsx_path, sheet_index = 1)\n",
    "    \n",
    "    # Split data\n",
    "    test_df, few_shot_df = split_data_into_sets(df, test_ratio=0.5, random_seed=random_seed)\n",
    "    \n",
    "    # Prepare results list\n",
    "    results = []\n",
    "    \n",
    "    # Process each test sentence\n",
    "    for idx, (_, row) in enumerate(test_df.iterrows(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing sentence {idx}/{len(test_df)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get column names\n",
    "        machine_col = None\n",
    "        human_col = None\n",
    "        \n",
    "        for col in row.index:\n",
    "            if 'machine' in col.lower() and 'transcription' in col.lower():\n",
    "                machine_col = col\n",
    "            elif 'human' in col.lower() and 'transcription' in col.lower():\n",
    "                human_col = col\n",
    "        \n",
    "        if machine_col is None or human_col is None:\n",
    "            # Fallback to first two columns\n",
    "            cols = list(row.index)\n",
    "            machine_col = cols[0] if machine_col is None else machine_col\n",
    "            human_col = cols[1] if human_col is None else human_col\n",
    "        \n",
    "        ground_truth = row.get(machine_col, \"\")\n",
    "        hypothesis = row.get(human_col, \"\")\n",
    "        \n",
    "        print(f\"\\nGround Truth: {ground_truth}\")\n",
    "        print(f\"Hypothesis: {hypothesis}\")\n",
    "        \n",
    "        # Get few-shot examples if needed\n",
    "        few_shot_examples = None\n",
    "        if few_shot:\n",
    "            few_shot_examples = format_few_shot_examples(few_shot_df, num_examples=5)\n",
    "            print(f\"\\nFew-shot Examples:\\n{few_shot_examples}\")\n",
    "        \n",
    "        # Build the prompt to print it\n",
    "        if few_shot and few_shot_examples:\n",
    "            examples_section = FEW_SHOT_EXAMPLES_TEMPLATE.format(examples=few_shot_examples)\n",
    "            full_prompt = FEW_SHOT_PROMPT.format(examples=examples_section, sentence=hypothesis)\n",
    "        else:\n",
    "            full_prompt = ZERO_SHOT_PROMPT.format(sentence=hypothesis)\n",
    "        \n",
    "        print(f\"\\nFull Input Prompt:\\n{full_prompt}\")\n",
    "        \n",
    "        # Classify\n",
    "        try:\n",
    "            raw_output, sentiment_label, reason = classify_with_llama(\n",
    "                hypothesis, \n",
    "                few_shot_examples=few_shot_examples,\n",
    "                few_shot=few_shot\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nModel Output:\\n{raw_output}\")\n",
    "            print(f\"\\nParsed Sentiment: {sentiment_label}\")\n",
    "            print(f\"Parsed Reason: {reason}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: {e}\")\n",
    "            raw_output = f\"ERROR: {e}\"\n",
    "            sentiment_label = None\n",
    "            reason = None\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'ground_truth': ground_truth,\n",
    "            'hypothesis': hypothesis,\n",
    "            'sentiment_label': sentiment_label if sentiment_label else \"ERROR\",\n",
    "            'reason': reason if reason else raw_output\n",
    "        })\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    shot_type = \"few_shot\" if few_shot else \"zero_shot\"\n",
    "    output_filename = f\"llama_2_13b_{shot_type}_{today}.csv\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = r\"data\\model_outputs\\llama_2_13b\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc99d8a",
   "metadata": {},
   "source": [
    "### Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497f71cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:12:58.635827Z",
     "start_time": "2025-11-26T00:12:58.630812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the path to your XLSX file\n",
    "xlsx_file_path = r\"C:\\Users\\pryce\\OneDrive\\Desktop\\Lost in Transcription\\Text Inputs\\Samples.xlsx\"  # Update this path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da5184",
   "metadata": {},
   "source": "###### Zero-shot (Split)\n"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ac3722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T00:13:06.392514Z",
     "start_time": "2025-11-26T00:13:00.742717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 124 rows from sheet 'Samples'\n",
      "INFO:root:Split data: 62 test samples, 62 few-shot examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing sentence 1/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Even when we come up from grade school, you know\n",
      "Hypothesis: Even when we coming up in grade school.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Even when we coming up in grade school.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 2/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Because you're supposed to give advice at home  first.\n",
      "Hypothesis: ‘Cause you supposed to get advice from home first.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: ‘Cause you supposed to get advice from home first.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 3/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Til I get up a little older, you know, and I see them next summer looking at me differently. I mean, I live up a little older, you know, and that's when you make some of them looking at me differently. I mean, I mean, I'm lookin at them skin tone and everything, but like I don't really want to say anything. You can sense things, feel things.\n",
      "Hypothesis: ‘Til I get up a little older, you know, and it seem like some of them looking at me differently; I’m looking at them, skin tone and everything, eyes, and you could sense things, feel things.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: ‘Til I get up a little older, you know, and it seem like some of them looking at me differently; I’m looking at them, skin tone and everything, eyes, and you could sense things, feel things.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 4/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know, when I can- he back- got back- got- he got his mind made about why he wanna do things. You know, I don't think about wanting to go down and get in and try to go behind him, you know.\n",
      "Hypothesis: But get his mind made up about how he going do things, and I don’t think anybody want to go down and try to go down behind him.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But get his mind made up about how he going do things, and I don’t think anybody want to go down and try to go down behind him.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 5/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: My family was the plantation of that church  and all all all of us from the genera generation we was raised down there at that church, First Providence.\n",
      "Hypothesis: My family was the plantation of that church, and all of us from generation to generations was raised down there in that church, First Providence.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: My family was the plantation of that church, and all of us from generation to generations was raised down there in that church, First Providence.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 6/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: But where I stay, r right around here, I don't have no trouble, no way, form, and fashion.\n",
      "Hypothesis: But where I stay right ‘round here, I don’t have no trouble no way, form, and fashion.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But where I stay right ‘round here, I don’t have no trouble no way, form, and fashion.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 7/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I think presidents, you know, the ones that are running the place, you know what I mean, I think they messed up in the head, you know what I'm saying.\n",
      "Hypothesis: The ones that running the place, they messed up in the head.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: The ones that running the place, they messed up in the head.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 8/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Oh, yeah. No problem with it.\n",
      "Hypothesis: Oh, they didn’t have no problem with it.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Oh, they didn’t have no problem with it.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 9/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I ain’t never, ain't never been prejudiced about me. Now, people, you sense it, they they they prejudiced against them n that, you know.\n",
      "Hypothesis: I ain’t never been prejudiced about me; now, people, you sense it, they prejudiced against me, you know.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I ain’t never been prejudiced about me; now, people, you sense it, they prejudiced against me, you know.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 10/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: They like that, you know, when I hear most of the Black people, you know, being like a young people. We was in the movement too, you know.\n",
      "Hypothesis: But now, here, most of the Black people, young people, we was in the Movement, too, you know.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But now, here, most of the Black people, young people, we was in the Movement, too, you know.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 11/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Basically, the blacks was going all the cooking restaurants and stuff, you know, but like,  do you as that, you know, that's far as it go.\n",
      "Hypothesis: Basically, Blacks were doing all the cooking at the restaurants and stuff, but that’s as far as it go.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Basically, Blacks were doing all the cooking at the restaurants and stuff, but that’s as far as it go.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 12/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: People rioting by throwing bricks in their house and stuff like that and stuff, you know, and old folk and stuff, you know, but like, you know, I'm pretty sure they was a little afraid of that, you know.\n",
      "Hypothesis: People rioting and throwing bricks at they house and stuff like that, and old folks and that, pretty sure they was afraid of that.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: People rioting and throwing bricks at they house and stuff like that, and old folks and that, pretty sure they was afraid of that.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 13/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: So I didn't don't have no have any education for us concern but I could do a little enough to take care of my business.\n",
      "Hypothesis: So I didn't—don't have any education far as concerned, but I could do enough to take care of my business.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: So I didn't—don't have any education far as concerned, but I could do enough to take care of my business.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 14/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I don't think they had no no no  so we have peoples over there, but I don't think they had much much, you know, they were just jealous. That's all I can say just jealous.\n",
      "Hypothesis: I don't think they had more, we have people who go over there but I don't think they had much, you know. They was just jealous, that's all I can say. They was just jealous.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I don't think they had more, we have people who go over there but I don't think they had much, you know. They was just jealous, that's all I can say. They was just jealous.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 15/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: No, that  was integration, man.\n",
      "Hypothesis: No, that back before integration, man.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: No, that back before integration, man.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 16/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know, stopped in the middle of the road and get ran over to a nigga- \"get in the backseat\" like that, you know\n",
      "Hypothesis: He ain’t going to stop there in the middle of the road and get ran over like, “Niggers be in the back seat.”\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: He ain’t going to stop there in the middle of the road and get ran over like, “Niggers be in the back seat.”\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 17/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Oh, that's my  my my husband. No, that's her daddy. Yeah, but you don't know because uh  after his first husband, I married her daddy.\n",
      "Hypothesis: Cause that's my first—my husband—no. That's her daddy. Yeah, but she don't know because after my first husband, I married her daddy.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Cause that's my first—my husband—no. That's her daddy. Yeah, but she don't know because after my first husband, I married her daddy.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 18/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: She was uh  uh   she was born in 1901 and I was born in 1930. So she's uh  what, twenty-nine  thirty years older than me.\n",
      "Hypothesis: She was born in 1901 and I was born in 1930, so she's twenty-nine or thirty years older than me.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: She was born in 1901 and I was born in 1930, so she's twenty-nine or thirty years older than me.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 19/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I don't know, it's a history of school burnings of a certain group of people, you know. So uh  they didn't only burn school, they burn churches too. So  that group of people so, you know, so I guess the Klu Kler Klan. Mm Hmm.\n",
      "Hypothesis: I don't know it's a history of school burnings of a certain group of people you know so they didn't only burn schools they burned churches too. So that group of people you know, I guess the Ku Klux Klan.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I don't know it's a history of school burnings of a certain group of people you know so they didn't only burn schools they burned churches too. So that group of people you know, I guess the Ku Klux Klan.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 20/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I mean like  I mean like  that all of them not the same, you know?\n",
      "Hypothesis: Just this idea, that all of them not the same, you know?\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Just this idea, that all of them not the same, you know?\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 21/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Look when we goin home, we walkin, you know.\n",
      "Hypothesis: But when we going home, we walking, you know?\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But when we going home, we walking, you know?\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 22/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Well, you know, I mean, turn out how things was back in the days, you know.\n",
      "Hypothesis: You know, I mean, turned out how things was back in the day.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: You know, I mean, turned out how things was back in the day.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 23/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know what I mean, like  mom saying  \"Come on, BD can't sit there!\"\n",
      "Hypothesis: My mom say, “No, come on, BT, you can’t sit there!”\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: My mom say, “No, come on, BT, you can’t sit there!”\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 24/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Wait, wait, wait a minute um  I know something about my granddaddy's parents.  A little bit uh  My great granddaddy  from my daddy's side  was Robert Godwin and I can't think of his wife now.\n",
      "Hypothesis: Wait a minute-I know something about my granddaddy's parents; a little bit. My great granddaddy on my daddy's side was Robert Godwin and I can't think of his wife right now.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Wait a minute-I know something about my granddaddy's parents; a little bit. My great granddaddy on my daddy's side was Robert Godwin and I can't think of his wife right now.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 25/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Oh, that is in the history. If I could find it, I would.   Here.    I can't remember. I mean, I can't uh  find it like it is in the history. If I could find that history, it would be the be easy for me to to  to follow.\n",
      "Hypothesis: All that is in the history, If could find it I would. I can't remember, if I could find that history it'd be easier for me to follow.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: All that is in the history, If could find it I would. I can't remember, if I could find that history it'd be easier for me to follow.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 26/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: We march Martin Luther King day every year, all up Citizen  Field, you know, it wa every year.\n",
      "Hypothesis: We marching every year for Martin Luther King uptown, all the way to Citizen Field, every year.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: We marching every year for Martin Luther King uptown, all the way to Citizen Field, every year.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 27/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know, you live to even open to open yourself up, you know, and and not being a really afraid to make a movement by something, you know, like  you know, no one could or wouldn't.\n",
      "Hypothesis: Now they was going to open yourself open and not really be afraid to make a movement about something.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Now they was going to open yourself open and not really be afraid to make a movement about something.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 28/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: And your country done deserted you, you know.\n",
      "Hypothesis: Your country done deserted you.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Your country done deserted you.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 29/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: White mom was in there looking at me like I'm trash. I said, \"I ain't going nowhere.\"\n",
      "Hypothesis: White folks in there looking at me like I’m shit I say, “I ain’t going nowhere.”\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: White folks in there looking at me like I’m shit I say, “I ain’t going nowhere.”\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 30/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: But working them odd every day and what, so it w it wasn’t nothing to me.\n",
      "Hypothesis: But working them odd every day, so it wasn’t nothing to me.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But working them odd every day, so it wasn’t nothing to me.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 31/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know what I mean, like  like in Ohio, Kent State, you know, when they're killing kids, you know, when the 1st black flag go down, they gonna shoot them kids up like that, you know.\n",
      "Hypothesis: Like, I know out in Kent State, they killed them kids, when the first Blacks going to go there they going shoot them kids up like that.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Like, I know out in Kent State, they killed them kids, when the first Blacks going to go there they going shoot them kids up like that.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 32/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Pretty much the old folks can they ain't have much to say about none to talk.\n",
      "Hypothesis: But pretty much, the old folks didn’t have much to say about nothing too talk.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But pretty much, the old folks didn’t have much to say about nothing too talk.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 33/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Somebody killing you that you don't know and like, you know what I mean, like um  it's a setback, you know, really\n",
      "Hypothesis: Somebody killing you that you don’t know, it's a setback.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Somebody killing you that you don’t know, it's a setback.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 34/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Still, prejudice is prejudice, you know, and b but I’m not go not gonna say nothing.\n",
      "Hypothesis: Still, prejudice is prejudice, you know, but I’m not going to say nothing.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Still, prejudice is prejudice, you know, but I’m not going to say nothing.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 35/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: In fact we didn't have a—our school building burned down when uh  I was in the fifth grade. So we didn't have a school. We had we went to went to school in churches in the rented houses.\n",
      "Hypothesis: In fact we didn't have a—our school building burned down when I was in the fifth grade, so we didn't have a school we had we went to school in churches and rented houses.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: In fact we didn't have a—our school building burned down when I was in the fifth grade, so we didn't have a school we had we went to school in churches and rented houses.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 36/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: But they they always gonna have some some people gonna say \"oh the west side and this and the west side and that.\"\n",
      "Hypothesis: But, they’re always gonna have some people gonna say, “Oh, the west is this, and the west side is that.”\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But, they’re always gonna have some people gonna say, “Oh, the west is this, and the west side is that.”\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 37/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You think things rough now but the thing was rough in Hoover time.  That's the best I can say cause  my folk we just, you know, you could go to the store. Long in that time, you go to the store and you could buy five c cent bag of peas for 10 cent.\n",
      "Hypothesis: You think things rough now but things were rough in Hoover time. That's the best I could say 'cause— you know you could go to the store and you could get you a bag of peas for ten cents.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: You think things rough now but things were rough in Hoover time. That's the best I could say 'cause— you know you could go to the store and you could get you a bag of peas for ten cents.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 38/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I don't have no problem with nobody, you know, cause like I never had no problem with n problem. Nobody else came on campus, you know.\n",
      "Hypothesis: Never had no problems with anybody; they stay on campus, you know?\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Never had no problems with anybody; they stay on campus, you know?\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 39/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I done been involved. Mm hmm.\n",
      "Hypothesis: I done been involved, some of it.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I done been involved, some of it.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 40/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: The old folk did too, but they wasn't no marching or nothing, you know what I mean.\n",
      "Hypothesis: The old folks did, too, but they didn’t do marching or nothing.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: The old folks did, too, but they didn’t do marching or nothing.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 41/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know, you listen enter them, you know, debates and stuff, you know, and see  I think what what they talking about, what they speaking of, you know, so you go with your choice, you know.\n",
      "Hypothesis: You listen to the debates and stuff, what he talking of, what he speaking of, so you vote for your choice.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: You listen to the debates and stuff, what he talking of, what he speaking of, so you vote for your choice.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 42/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: And  I think I was born in  May the 30th in 191912 or like  1908. I think that's the way they really got it. May the 30th, 1908. And you were born in Tallahassee?\n",
      "Hypothesis: I think I was born May 30, 1912 or 1908. I think that's the way they got it. May 30, 1908? And you were born in Tallahassee?\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I think I was born May 30, 1912 or 1908. I think that's the way they got it. May 30, 1908? And you were born in Tallahassee?\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 43/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I never been in jail my whole life, you know, I ain't ha had no problems\n",
      "Hypothesis: I never been in jail my whole life, I ain’t had no problems.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I never been in jail my whole life, I ain’t had no problems.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 44/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: So but we, we  we had a wonderful school, you know, wonderful teachers and principal Mr  uh uh  Homer Smith was my principal.  So.\n",
      "Hypothesis: So, but we had a wonderful school you know, wonderful teachers and principal Mr. Homer Smith was my principal so.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: So, but we had a wonderful school you know, wonderful teachers and principal Mr. Homer Smith was my principal so.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 45/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: My people weren't able to buy me a bicycle.\n",
      "Hypothesis: My people wasn’t able to buy me a bicycle.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: My people wasn’t able to buy me a bicycle.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 46/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Because it was like uh   the whole community was a family. And everybody looked after everybody. Everybody was everybody's mother and father. And so  that's the love of the people. That's that's what I like about it   so I feel like about it.\n",
      "Hypothesis: Because it was like the whole community was a family and everybody looked after everybody. Everybody was everybody's mother and father and so that's the love of the people, that's what I liked about it. So—And that's what I still liked about it.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Because it was like the whole community was a family and everybody looked after everybody. Everybody was everybody's mother and father and so that's the love of the people, that's what I liked about it. So—And that's what I still liked about it.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 47/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I didn't have no problem. I don't even know quite- basically basically I did a lot of work here on camp, you know, like roofing.\n",
      "Hypothesis: I didn’t have no problems, because basically, get a lots of work there on campus.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: I didn’t have no problems, because basically, get a lots of work there on campus.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 48/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I don't think that that don't stop my dollar bill. I got to work, you know.\n",
      "Hypothesis: That don’t stop my dollar bill; I got to work, you know?\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: That don’t stop my dollar bill; I got to work, you know?\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 49/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: It was just just just in my generation, that's the way it run.\n",
      "Hypothesis: In my generation that’s the way it run.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: In my generation that’s the way it run.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 50/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: And whatever they do looks like it it's  alright with them.\n",
      "Hypothesis: And whatever they do look like it’s alright with them.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: And whatever they do look like it’s alright with them.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 51/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Ain't like like everybody get together, all together. All the black people like Blacks and them.\n",
      "Hypothesis: It ain’t like everybody get together, all the Blacks and them.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: It ain’t like everybody get together, all the Blacks and them.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 52/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: But with wasn't no violent or nothing, you know, but like we was standing on everything, you know what I mean.\n",
      "Hypothesis: But wasn’t no violence or nothing; but we was standing and everything.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But wasn’t no violence or nothing; but we was standing and everything.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 53/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: So  I want to know in your own words, what was it that, what is it that, that, that makes a person a leader? What is leadership?\n",
      "Hypothesis: So I wanted to know in your own words what was it that, what is it that makes a person a leader? What is leadership?\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: So I wanted to know in your own words what was it that, what is it that makes a person a leader? What is leadership?\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 54/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: I'm gonna think about it, go out there and fight for the country, you know, come back, you know, but I'm going to think about it, go out there, fight for your country, you know, come back, you know, and  can't find no job or nothing to do or whatever, you know.\n",
      "Hypothesis: But I can think about that, going and fight for your country, coming out, can’t find no jobs.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: But I can think about that, going and fight for your country, coming out, can’t find no jobs.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 55/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: So uh  from six years old up until I married out of the house in 1950 uh  you know, I lived with her.\n",
      "Hypothesis: So from six years old up until I married out of the house in 1950, I lived with her.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: So from six years old up until I married out of the house in 1950, I lived with her.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 56/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: So w we didn't know really know who it was because they was covered in sheets  and hoods. You didn't know  who they were.\n",
      "Hypothesis: So we didn't really know who it was because they was covered in sheets and hoods. We didn't know who they was.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: So we didn't really know who it was because they was covered in sheets and hoods. We didn't know who they was.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 57/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know  like  they didn't want to have anything to do with- simply because they were old and stuff and they scared of the change and stuff.\n",
      "Hypothesis: Because they didn’t want to have anything to do with — seemed like because they old and stuff, and scared of the change.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: Because they didn’t want to have anything to do with — seemed like because they old and stuff, and scared of the change.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 58/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: What-what was your major? I was in the College of Pharmacy. Oh wow. That's good. I could not handle it.\n",
      "Hypothesis: What was your major? I was in the College of Pharmacy. Oh, wow! That's good? I could not handle it.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: What was your major? I was in the College of Pharmacy. Oh, wow! That's good? I could not handle it.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 59/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: You know what I mean like anything but  it wasn't no violent thing, you know, just getting together, you know what I mean, like discussing, you know, debating about this and that, you know\n",
      "Hypothesis: And you think about, it really wasn’t no violent thing, we was just getting together, discussing and that.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: And you think about, it really wasn’t no violent thing, we was just getting together, discussing and that.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 60/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: There was a time used to be your door opening and and have to lock it or nothin.\n",
      "Hypothesis: There was a time you used to leave your door open and didn’t have to lock it or nothing.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: There was a time you used to leave your door open and didn’t have to lock it or nothing.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 61/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Well, that was fine with me, you know, because it's like you say, it will change. Time will change. Like I'm really now- I don't think that I really want to be this in this position.\n",
      "Hypothesis: That was fine with me, because like I said, when change happen, change — like, really now, I don’t think anybody want to be in his position.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: That was fine with me, because like I said, when change happen, change — like, really now, I don’t think anybody want to be in his position.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Processing sentence 62/62\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: The greatest thing about going to UF. Well    uh     That'll take some thought   it  know what it was not as  I noticed as the years even in 1976 when I finished high school, the reputation that and I think even more so now would probably even more so or about the same, the reputation of this university is has always been one of them that is uh  pretty intimidating.\n",
      "Hypothesis: The greatest thing about going to UF Well, that'll take some thought. Now, when it was not as—I notice as the years—even in 1976, when I finished high school, the reputation that—and yeah, I think even more so now; well, probably even more so, or about the same—the reputation of this University has always been one that is pretty intimidating.\n",
      "\n",
      "Full Input Prompt:\n",
      "\n",
      "<<SYS>>\n",
      "You are an assistant that classifies the sentiment of user utterances.  You must respond with three parts:\n",
      "1) A single label: `Positive`, `Negative`, or `Neutral`\n",
      "2) A short explanation (1–2 sentences) of why you chose that label\n",
      "3) Format your response as follows: [Sentiment: <label>, Reason: <explanation>]\n",
      "4) (Optionally) any caveats or uncertainty if applicable\n",
      "<</SYS>>\n",
      "[INST]\n",
      "User: The greatest thing about going to UF Well, that'll take some thought. Now, when it was not as—I notice as the years—even in 1976, when I finished high school, the reputation that—and yeah, I think even more so now; well, probably even more so, or about the same—the reputation of this University has always been one that is pretty intimidating.\n",
      "[/INST]\n",
      "Assistant:\n",
      "\n",
      "\n",
      "ERROR: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "================================================================================\n",
      "Results saved to: data/model_outputs/llama_2_13b\\llama_2_13b_zero_shot_20251125.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run zero-shot sentiment analysis\n",
    "zero_shot_results = process_sentiment_analysis(\n",
    "    xlsx_file_path, \n",
    "    few_shot = False,\n",
    "    random_seed = RANDOM_SEED\n",
    ")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Zero-Shot (No Split)",
   "id": "7aba084523e20f85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_samples_zero_shot_results = process_all_samples_zero_shot(xlsx_file_path)",
   "id": "956cf7dc33fe818"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Few-shot\n",
   "id": "6fd9103c92096996"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run few-shot sentiment analysis\n",
    "few_shot_results = process_sentiment_analysis(\n",
    "    xlsx_file_path, \n",
    "    few_shot = True,\n",
    "    random_seed = RANDOM_SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e4f37745b76bc",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f7d02972e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = r\"data/model_outputs/llama_2_13b\"\n",
    "data_path = r\"data/reference\"\n",
    "input_files = []  # include \".csv\" in the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8dd22a8ceffdab",
   "metadata": {},
   "source": [
    "###### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e24c7a3e7aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run zero-shot over all csv files in input_files\n",
    "zs_results = []\n",
    "for file in input_files:\n",
    "    print(\"Processing zero-shot on:\", file)\n",
    "    path = os.path.join(data_path, file)\n",
    "    classifications = run_sentiment_on_csv(path, \"Sentences\", few_shot = False)\n",
    "    zs_results.append(classifications)\n",
    "\n",
    "for file_name, classifications in zip(input_files, zs_results):\n",
    "    out_file = os.path.join(save_directory, file_name)\n",
    "    classifications.to_csv(out_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec882521dc68d531",
   "metadata": {},
   "source": [
    "###### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6458548fd38c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_results = []\n",
    "for file in input_files:\n",
    "    print(\"Processing few-shot on:\", file)\n",
    "    path = os.path.join(data_path, file)\n",
    "    classifications = run_sentiment_on_csv(path, \"Sentences\", few_shot = True)\n",
    "    fs_results.append(classifications)\n",
    "\n",
    "for file_name, classifications in zip(input_files, fs_results):\n",
    "    out_file = os.path.join(save_directory, file_name)\n",
    "    classifications.to_csv(out_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
